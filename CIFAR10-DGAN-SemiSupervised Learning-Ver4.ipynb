{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrafi123/anaconda3/lib/python3.5/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce 920M (0000:08:00.0)\n"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "#os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu0,floatX=float32\"\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=cuda,floatX=float32\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, UpSampling3D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "\n",
    "img_rows = 32\n",
    "img_cols = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<GpuArrayType<None>(float32, (False,))>), HostFromGpu(gpuarray)(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.896530 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "#test for gpu\n",
    "from theano import function, config, shared, tensor\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    # bulid the generator model, it is a model made up of UpSample and Convolution\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, output_dim=2048, init='normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Dense(2048))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    #model.add(Reshape((2, 2, 512), input_shape=(2048,))) commented by me next line added by me\n",
    "    model.add(Reshape((512, 2, 2), input_shape=(2048,)))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(256, 5, 5, border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(128, 5, 5, border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Convolution2D(3, 5, 5, border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    print ('Generator model...')\n",
    "    print (model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def Discriminator():\n",
    "    model = Sequential()\n",
    "    #model.add(Convolution2D(64, 5, 5, border_mode='same',subsample=(2, 2), input_shape=(32, 32, 3))) commented by me next line added by me\n",
    "    model.add(Convolution2D(64, 5, 5, border_mode='same',subsample=(2, 2), input_shape=(3, 32, 32)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    #model.add(Activation('tanh'))\n",
    "    model.add(Convolution2D(128, 5, 5, border_mode='same', subsample=(2,2)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    #model.add(Activation('tanh'))\n",
    "    model.add(Convolution2D(256, 5, 5, border_mode='same', subsample=(2,2)))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    #model.add(Activation('tanh'))\n",
    "    model.add(Convolution2D(512, 5, 5, border_mode='same', subsample=(4,4)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    #model.add(Activation('tanh'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    print ('Discriminator model...')\n",
    "\n",
    "    print (model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "def generator_containing_discriminator(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "# Note that you will have to change the output_shape depending on the backend used.\n",
    "\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    print ('num ',num)\n",
    "    width = int(math.sqrt(num))\n",
    "    print ('width ',width)\n",
    "    height = int(math.ceil(float(num)/width))\n",
    "    print ('height ',height)\n",
    "    print ('generated images shape before reshape ',generated_images.shape)\n",
    "    ## added the below line to overcome the poor quality of image generated issue with RGB channel \n",
    "    generated_images = generated_images.reshape(generated_images.shape[0], 3, 32, 32).transpose(0,2,3,1)\n",
    "\n",
    "    generated_images = generated_images.reshape((generated_images.shape[0], 32, 32, 3) )\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1], 3),dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1], 0] = \\\n",
    "            img[:, :, 0]\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1], 1] = \\\n",
    "            img[:, :, 1]\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1], 2] = \\\n",
    "            img[:, :, 2]\n",
    "    return image\n",
    "\n",
    "\n",
    "def train(BATCH_SIZE, epoch_num):\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "    #print X_train.dtype\n",
    "    #X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    #print X_train.dtype\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    #X_train /= 255\n",
    "    X_train = (X_train - 127.5)/127.5\n",
    "    X_test /= 255\n",
    "    X_train = X_train.reshape((X_train.shape[0], ) + X_train.shape[1:])\n",
    "    #X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "    print ('X_train shape ',X_train.shape)\n",
    "    print ('X_test shape ', X_test.shape)\n",
    "    discriminator = Discriminator()\n",
    "    print ('Discriminator initialized...')\n",
    "    generator = Generator()\n",
    "    print ('Generator initialized...')\n",
    "\n",
    "    discriminator_on_generator = generator_containing_discriminator(generator, discriminator)\n",
    "    print ('generator_containing_discriminator initialized...')\n",
    "    \n",
    "    #d_optim = SGD(lr=0.0005, momentum=0.5, nesterov=True)\n",
    "    #g_optim = SGD(lr=0.0005, momentum=0.5, nesterov=True)\n",
    "    d_optim = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    g_optim = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"Adam\")\n",
    "    discriminator_on_generator.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    print ('noise shape ',noise.shape)\n",
    "    #for epoch in range(100):\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "    \n",
    "        batches_num = int(X_train.shape[0]/BATCH_SIZE)\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        # load weights on first try (i.e. if process failed previously and we are attempting to recapture lost data)\n",
    "        \n",
    "        if epoch == 0:\n",
    "            if os.path.exists('generator_cifar') and os.path.exists('discriminator_cifar'):\n",
    "                print (\"Loading saves weights..\")\n",
    "                generator.load_weights('generator_cifar')\n",
    "                discriminator.load_weights('discriminator_cifar')\n",
    "                print (\"Finished loading\")\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        for index in range(batches_num):\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "                #print 'noise', noise.dtype\n",
    "            discriminator.trainable = False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(noise, [0.9] * BATCH_SIZE)\n",
    "            \n",
    "            #filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "            #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "            #callbacks_list = [checkpoint]\n",
    "            \n",
    "            discriminator.trainable = True\n",
    "            print(\"epoch %d/%d batch %d/%d g_loss : %f\" % (epoch+1, epoch_num,index, batches_num, g_loss))\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            \n",
    "            image_batch = X_train[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
    "            generated_images = generator.predict(noise, verbose=0)\n",
    "            \n",
    "            if index % 64 == 0:\n",
    "                gen_image = combine_images(generated_images)\n",
    "                gen_image = gen_image.reshape(384, 352, 3)\n",
    "                gen_image = gen_image*127.5+127.5\n",
    "                Image.fromarray(gen_image.astype(np.uint8)).save('images/gen_image'+\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "\n",
    "                org_image = combine_images(image_batch)\n",
    "                org_image = org_image.reshape(384, 352, 3)\n",
    "                org_image = org_image*127.5+127.5\n",
    "                Image.fromarray(org_image.astype(np.uint8)).save('images/org_image'+\n",
    "                    str(epoch)+\"_\"+str(index)+\".png\")\n",
    "\n",
    "                #print image_batch.shape, generated_images.shape\n",
    "\n",
    "            X = np.concatenate((image_batch, generated_images))\n",
    "            print ('train images X shape before transpose',X.shape)\n",
    "            #magic!??\n",
    "            X = X.reshape(X.shape[0], 3, 32, 32).transpose(0,2,3,1)\n",
    "            print ('train images X shape after transpose',X.shape)\n",
    "            \n",
    "            y = [0.9] * BATCH_SIZE + [0.0] * BATCH_SIZE\n",
    "            #y = np.array(y)\n",
    "            #print 'y ', y.shape\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            print(\"epoch %d/%d batch %d/%d d_loss : %f\" % (epoch+1, epoch_num, index, batches_num, d_loss))\n",
    "            #for i in range(BATCH_SIZE):\n",
    "            #    noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "            #discriminator.trainable = False\n",
    "            '''g_loss = discriminator_on_generator.train_on_batch(\n",
    "                noise, np.array([1.0] * BATCH_SIZE))\n",
    "            discriminator.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            '''\n",
    "            if index % 20 == 0:\n",
    "                generator.save_weights('generator_cifar', True)\n",
    "                discriminator.save_weights('discriminator_cifar', True)\n",
    "\n",
    "'''\n",
    "def generate(BATCH_SIZE, nice=False):\n",
    "    generator = Generator()\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=\"Adam\")\n",
    "    generator.load_weights('generator_cifar')\n",
    "    if nice:\n",
    "        discriminator = Discriminator()\n",
    "        discriminator.compile(loss='binary_crossentropy', optimizer=\"Adam\")\n",
    "        discriminator.load_weights('discriminator_cifar')\n",
    "        noise = np.zeros((BATCH_SIZE*20, 100))\n",
    "        for i in range(BATCH_SIZE*20):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        print generated_images.shape\n",
    "        d_pret = discriminator.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True)\n",
    "        nice_images = np.zeros((BATCH_SIZE, ) +\n",
    "                           (generated_images.shape[1:3]) + (1,), dtype=np.float32)\n",
    "        for i in range(int(BATCH_SIZE)):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.zeros((BATCH_SIZE, 100))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            noise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "        generated_images = generator.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    #image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        \"generated_image.png\")\n",
    "'''\n",
    "\n",
    "def generate(BATCH_SIZE):\n",
    "    generator = Generator()\n",
    "    d_optim = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    g_optim = Adam(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    generator.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    generator.load_weights('generator_cifar')\n",
    "    noise = np.zeros((BATCH_SIZE, 100))\n",
    "    for i in range(BATCH_SIZE):\n",
    "    \tnoise[i, :] = np.random.uniform(-1, 1, 100)\n",
    "    generated_images = generator.predict(noise, verbose=1)\n",
    "    print ('after generator predict generated images shape',generated_images.shape)\n",
    "    image = combine_images(generated_images)\n",
    "\n",
    "    image = image.reshape(384, 352, 3)\n",
    "\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"generated_image.png\")\n",
    "\n",
    "    #clr_img = Image.fromarray(image,'RGB')   \n",
    "    #clr_img.save(\"generated_image_clr.png\")\n",
    "    \n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.add_argument(\"--epoch_num\",type=int,default=100)\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if not os.path.exists('images'):\n",
    "        os.mkdir('images')\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        print ('totol epochs of the train:'+str(args.epoch_num))\n",
    "        train(BATCH_SIZE=args.batch_size, epoch_num=args.epoch_num)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('images'):\n",
    "    os.mkdir('images')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (50000, 3, 32, 32)\n",
      "X_test shape  (10000, 3, 32, 32)\n",
      "Discriminator model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_13 (Convolution2D) (None, 64, 16, 16)    4864        convolution2d_input_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_5 (LeakyReLU)          (None, 64, 16, 16)    0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 128, 8, 8)     204928      leakyrelu_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_6 (LeakyReLU)          (None, 128, 8, 8)     0           convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 256, 4, 4)     819456      leakyrelu_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_7 (LeakyReLU)          (None, 256, 4, 4)     0           convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 512, 1, 1)     3277312     leakyrelu_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_8 (LeakyReLU)          (None, 512, 1, 1)     0           convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 512)           0           leakyrelu_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             513         flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 1)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,307,073\n",
      "Trainable params: 4,307,073\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Discriminator initialized...\n",
      "Generator model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_5 (Dense)                  (None, 2048)          206848      dense_input_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 2048)          8192        dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, 2048)          0           batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)              (None, 512, 2, 2)     0           activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_9 (UpSampling2D)    (None, 512, 4, 4)     0           reshape_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_17 (Convolution2D) (None, 256, 4, 4)     3277056     upsampling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (None, 256, 4, 4)     16          convolution2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, 256, 4, 4)     0           batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_10 (UpSampling2D)   (None, 256, 8, 8)     0           activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_18 (Convolution2D) (None, 128, 8, 8)     819328      upsampling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_11 (BatchNorm (None, 128, 8, 8)     32          convolution2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, 128, 8, 8)     0           batchnormalization_11[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_11 (UpSampling2D)   (None, 128, 16, 16)   0           activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_19 (Convolution2D) (None, 64, 16, 16)    204864      upsampling2d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_12 (BatchNorm (None, 64, 16, 16)    64          convolution2d_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, 64, 16, 16)    0           batchnormalization_12[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_12 (UpSampling2D)   (None, 64, 32, 32)    0           activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_20 (Convolution2D) (None, 3, 32, 32)     4803        upsampling2d_12[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, 3, 32, 32)     0           convolution2d_20[0][0]           \n",
      "====================================================================================================\n",
      "Total params: 4,521,203\n",
      "Trainable params: 4,517,051\n",
      "Non-trainable params: 4,152\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Generator initialized...\n",
      "generator_containing_discriminator initialized...\n",
      "noise shape  (128, 100)\n",
      "Epoch is 0\n",
      "Number of batches 390\n",
      "Loading saves weights..\n",
      "Finished loading\n",
      "epoch 1/2 batch 0/390 g_loss : 2.869165\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 0/390 d_loss : 0.309709\n",
      "epoch 1/2 batch 1/390 g_loss : 1.510640\n",
      "epoch 1/2 batch 1/390 d_loss : 0.697736\n",
      "epoch 1/2 batch 2/390 g_loss : 2.580252\n",
      "epoch 1/2 batch 2/390 d_loss : 0.339525\n",
      "epoch 1/2 batch 3/390 g_loss : 3.074452\n",
      "epoch 1/2 batch 3/390 d_loss : 0.359443\n",
      "epoch 1/2 batch 4/390 g_loss : 2.647300\n",
      "epoch 1/2 batch 4/390 d_loss : 0.389817\n",
      "epoch 1/2 batch 5/390 g_loss : 1.895111\n",
      "epoch 1/2 batch 5/390 d_loss : 0.400680\n",
      "epoch 1/2 batch 6/390 g_loss : 2.538303\n",
      "epoch 1/2 batch 6/390 d_loss : 0.347883\n",
      "epoch 1/2 batch 7/390 g_loss : 2.348380\n",
      "epoch 1/2 batch 7/390 d_loss : 0.365390\n",
      "epoch 1/2 batch 8/390 g_loss : 2.328215\n",
      "epoch 1/2 batch 8/390 d_loss : 0.373751\n",
      "epoch 1/2 batch 9/390 g_loss : 2.257791\n",
      "epoch 1/2 batch 9/390 d_loss : 0.328998\n",
      "epoch 1/2 batch 10/390 g_loss : 2.481528\n",
      "epoch 1/2 batch 10/390 d_loss : 0.400536\n",
      "epoch 1/2 batch 11/390 g_loss : 2.037974\n",
      "epoch 1/2 batch 11/390 d_loss : 0.386141\n",
      "epoch 1/2 batch 12/390 g_loss : 2.120278\n",
      "epoch 1/2 batch 12/390 d_loss : 0.323008\n",
      "epoch 1/2 batch 13/390 g_loss : 2.716669\n",
      "epoch 1/2 batch 13/390 d_loss : 0.377413\n",
      "epoch 1/2 batch 14/390 g_loss : 2.160112\n",
      "epoch 1/2 batch 14/390 d_loss : 0.345598\n",
      "epoch 1/2 batch 15/390 g_loss : 2.522457\n",
      "epoch 1/2 batch 15/390 d_loss : 0.384208\n",
      "epoch 1/2 batch 16/390 g_loss : 2.933819\n",
      "epoch 1/2 batch 16/390 d_loss : 0.469088\n",
      "epoch 1/2 batch 17/390 g_loss : 1.856816\n",
      "epoch 1/2 batch 17/390 d_loss : 0.380510\n",
      "epoch 1/2 batch 18/390 g_loss : 2.377908\n",
      "epoch 1/2 batch 18/390 d_loss : 0.353123\n",
      "epoch 1/2 batch 19/390 g_loss : 2.586798\n",
      "epoch 1/2 batch 19/390 d_loss : 0.351859\n",
      "epoch 1/2 batch 20/390 g_loss : 2.367200\n",
      "epoch 1/2 batch 20/390 d_loss : 0.494570\n",
      "epoch 1/2 batch 21/390 g_loss : 2.856516\n",
      "epoch 1/2 batch 21/390 d_loss : 0.360395\n",
      "epoch 1/2 batch 22/390 g_loss : 2.511019\n",
      "epoch 1/2 batch 22/390 d_loss : 0.444723\n",
      "epoch 1/2 batch 23/390 g_loss : 2.506949\n",
      "epoch 1/2 batch 23/390 d_loss : 0.410846\n",
      "epoch 1/2 batch 24/390 g_loss : 2.137440\n",
      "epoch 1/2 batch 24/390 d_loss : 0.371973\n",
      "epoch 1/2 batch 25/390 g_loss : 2.058481\n",
      "epoch 1/2 batch 25/390 d_loss : 0.355444\n",
      "epoch 1/2 batch 26/390 g_loss : 2.438365\n",
      "epoch 1/2 batch 26/390 d_loss : 0.403521\n",
      "epoch 1/2 batch 27/390 g_loss : 2.422371\n",
      "epoch 1/2 batch 27/390 d_loss : 0.336399\n",
      "epoch 1/2 batch 28/390 g_loss : 2.141405\n",
      "epoch 1/2 batch 28/390 d_loss : 0.384209\n",
      "epoch 1/2 batch 29/390 g_loss : 2.284539\n",
      "epoch 1/2 batch 29/390 d_loss : 0.328627\n",
      "epoch 1/2 batch 30/390 g_loss : 2.800646\n",
      "epoch 1/2 batch 30/390 d_loss : 0.365625\n",
      "epoch 1/2 batch 31/390 g_loss : 2.224734\n",
      "epoch 1/2 batch 31/390 d_loss : 0.363342\n",
      "epoch 1/2 batch 32/390 g_loss : 2.292310\n",
      "epoch 1/2 batch 32/390 d_loss : 0.389085\n",
      "epoch 1/2 batch 33/390 g_loss : 2.392473\n",
      "epoch 1/2 batch 33/390 d_loss : 0.348291\n",
      "epoch 1/2 batch 34/390 g_loss : 2.189415\n",
      "epoch 1/2 batch 34/390 d_loss : 0.408620\n",
      "epoch 1/2 batch 35/390 g_loss : 3.087870\n",
      "epoch 1/2 batch 35/390 d_loss : 0.424580\n",
      "epoch 1/2 batch 36/390 g_loss : 1.709805\n",
      "epoch 1/2 batch 36/390 d_loss : 0.458792\n",
      "epoch 1/2 batch 37/390 g_loss : 2.586449\n",
      "epoch 1/2 batch 37/390 d_loss : 0.346590\n",
      "epoch 1/2 batch 38/390 g_loss : 1.847548\n",
      "epoch 1/2 batch 38/390 d_loss : 0.439575\n",
      "epoch 1/2 batch 39/390 g_loss : 2.742630\n",
      "epoch 1/2 batch 39/390 d_loss : 0.458031\n",
      "epoch 1/2 batch 40/390 g_loss : 1.852628\n",
      "epoch 1/2 batch 40/390 d_loss : 0.407368\n",
      "epoch 1/2 batch 41/390 g_loss : 2.714935\n",
      "epoch 1/2 batch 41/390 d_loss : 0.435587\n",
      "epoch 1/2 batch 42/390 g_loss : 1.747705\n",
      "epoch 1/2 batch 42/390 d_loss : 0.378298\n",
      "epoch 1/2 batch 43/390 g_loss : 2.701471\n",
      "epoch 1/2 batch 43/390 d_loss : 0.327302\n",
      "epoch 1/2 batch 44/390 g_loss : 2.572072\n",
      "epoch 1/2 batch 44/390 d_loss : 0.360079\n",
      "epoch 1/2 batch 45/390 g_loss : 2.527843\n",
      "epoch 1/2 batch 45/390 d_loss : 0.402708\n",
      "epoch 1/2 batch 46/390 g_loss : 2.582750\n",
      "epoch 1/2 batch 46/390 d_loss : 0.371459\n",
      "epoch 1/2 batch 47/390 g_loss : 2.938910\n",
      "epoch 1/2 batch 47/390 d_loss : 0.288031\n",
      "epoch 1/2 batch 48/390 g_loss : 3.078417\n",
      "epoch 1/2 batch 48/390 d_loss : 0.383645\n",
      "epoch 1/2 batch 49/390 g_loss : 2.700358\n",
      "epoch 1/2 batch 49/390 d_loss : 0.341373\n",
      "epoch 1/2 batch 50/390 g_loss : 2.704628\n",
      "epoch 1/2 batch 50/390 d_loss : 0.393003\n",
      "epoch 1/2 batch 51/390 g_loss : 2.854435\n",
      "epoch 1/2 batch 51/390 d_loss : 0.383836\n",
      "epoch 1/2 batch 52/390 g_loss : 2.396410\n",
      "epoch 1/2 batch 52/390 d_loss : 0.388312\n",
      "epoch 1/2 batch 53/390 g_loss : 2.234481\n",
      "epoch 1/2 batch 53/390 d_loss : 0.331713\n",
      "epoch 1/2 batch 54/390 g_loss : 2.717097\n",
      "epoch 1/2 batch 54/390 d_loss : 0.358957\n",
      "epoch 1/2 batch 55/390 g_loss : 2.139507\n",
      "epoch 1/2 batch 55/390 d_loss : 0.345443\n",
      "epoch 1/2 batch 56/390 g_loss : 2.161859\n",
      "epoch 1/2 batch 56/390 d_loss : 0.396915\n",
      "epoch 1/2 batch 57/390 g_loss : 2.872048\n",
      "epoch 1/2 batch 57/390 d_loss : 0.397326\n",
      "epoch 1/2 batch 58/390 g_loss : 2.375738\n",
      "epoch 1/2 batch 58/390 d_loss : 0.373278\n",
      "epoch 1/2 batch 59/390 g_loss : 2.466268\n",
      "epoch 1/2 batch 59/390 d_loss : 0.431131\n",
      "epoch 1/2 batch 60/390 g_loss : 3.890485\n",
      "epoch 1/2 batch 60/390 d_loss : 0.409207\n",
      "epoch 1/2 batch 61/390 g_loss : 1.844866\n",
      "epoch 1/2 batch 61/390 d_loss : 0.637371\n",
      "epoch 1/2 batch 62/390 g_loss : 3.855670\n",
      "epoch 1/2 batch 62/390 d_loss : 0.604731\n",
      "epoch 1/2 batch 63/390 g_loss : 1.200156\n",
      "epoch 1/2 batch 63/390 d_loss : 0.618161\n",
      "epoch 1/2 batch 64/390 g_loss : 3.214719\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 64/390 d_loss : 0.468579\n",
      "epoch 1/2 batch 65/390 g_loss : 2.328916\n",
      "epoch 1/2 batch 65/390 d_loss : 0.409340\n",
      "epoch 1/2 batch 66/390 g_loss : 2.010351\n",
      "epoch 1/2 batch 66/390 d_loss : 0.416053\n",
      "epoch 1/2 batch 67/390 g_loss : 2.812979\n",
      "epoch 1/2 batch 67/390 d_loss : 0.443319\n",
      "epoch 1/2 batch 68/390 g_loss : 1.866753\n",
      "epoch 1/2 batch 68/390 d_loss : 0.423810\n",
      "epoch 1/2 batch 69/390 g_loss : 1.697050\n",
      "epoch 1/2 batch 69/390 d_loss : 0.415647\n",
      "epoch 1/2 batch 70/390 g_loss : 2.442365\n",
      "epoch 1/2 batch 70/390 d_loss : 0.385246\n",
      "epoch 1/2 batch 71/390 g_loss : 2.654103\n",
      "epoch 1/2 batch 71/390 d_loss : 0.388208\n",
      "epoch 1/2 batch 72/390 g_loss : 2.284978\n",
      "epoch 1/2 batch 72/390 d_loss : 0.350929\n",
      "epoch 1/2 batch 73/390 g_loss : 2.368900\n",
      "epoch 1/2 batch 73/390 d_loss : 0.399282\n",
      "epoch 1/2 batch 74/390 g_loss : 2.659051\n",
      "epoch 1/2 batch 74/390 d_loss : 0.370123\n",
      "epoch 1/2 batch 75/390 g_loss : 2.496794\n",
      "epoch 1/2 batch 75/390 d_loss : 0.362547\n",
      "epoch 1/2 batch 76/390 g_loss : 2.379577\n",
      "epoch 1/2 batch 76/390 d_loss : 0.387110\n",
      "epoch 1/2 batch 77/390 g_loss : 1.833769\n",
      "epoch 1/2 batch 77/390 d_loss : 0.431317\n",
      "epoch 1/2 batch 78/390 g_loss : 2.921597\n",
      "epoch 1/2 batch 78/390 d_loss : 0.366090\n",
      "epoch 1/2 batch 79/390 g_loss : 2.313872\n",
      "epoch 1/2 batch 79/390 d_loss : 0.361656\n",
      "epoch 1/2 batch 80/390 g_loss : 2.457029\n",
      "epoch 1/2 batch 80/390 d_loss : 0.360588\n",
      "epoch 1/2 batch 81/390 g_loss : 2.440743\n",
      "epoch 1/2 batch 81/390 d_loss : 0.357912\n",
      "epoch 1/2 batch 82/390 g_loss : 2.363888\n",
      "epoch 1/2 batch 82/390 d_loss : 0.363111\n",
      "epoch 1/2 batch 83/390 g_loss : 2.208549\n",
      "epoch 1/2 batch 83/390 d_loss : 0.375165\n",
      "epoch 1/2 batch 84/390 g_loss : 2.569221\n",
      "epoch 1/2 batch 84/390 d_loss : 0.354606\n",
      "epoch 1/2 batch 85/390 g_loss : 2.655898\n",
      "epoch 1/2 batch 85/390 d_loss : 0.410683\n",
      "epoch 1/2 batch 86/390 g_loss : 2.197612\n",
      "epoch 1/2 batch 86/390 d_loss : 0.353735\n",
      "epoch 1/2 batch 87/390 g_loss : 2.446305\n",
      "epoch 1/2 batch 87/390 d_loss : 0.373119\n",
      "epoch 1/2 batch 88/390 g_loss : 2.345190\n",
      "epoch 1/2 batch 88/390 d_loss : 0.335738\n",
      "epoch 1/2 batch 89/390 g_loss : 2.268521\n",
      "epoch 1/2 batch 89/390 d_loss : 0.383722\n",
      "epoch 1/2 batch 90/390 g_loss : 2.816246\n",
      "epoch 1/2 batch 90/390 d_loss : 0.472394\n",
      "epoch 1/2 batch 91/390 g_loss : 1.786275\n",
      "epoch 1/2 batch 91/390 d_loss : 0.386919\n",
      "epoch 1/2 batch 92/390 g_loss : 2.338605\n",
      "epoch 1/2 batch 92/390 d_loss : 0.345896\n",
      "epoch 1/2 batch 93/390 g_loss : 2.507881\n",
      "epoch 1/2 batch 93/390 d_loss : 0.340246\n",
      "epoch 1/2 batch 94/390 g_loss : 2.495797\n",
      "epoch 1/2 batch 94/390 d_loss : 0.317748\n",
      "epoch 1/2 batch 95/390 g_loss : 2.399844\n",
      "epoch 1/2 batch 95/390 d_loss : 0.375913\n",
      "epoch 1/2 batch 96/390 g_loss : 2.386805\n",
      "epoch 1/2 batch 96/390 d_loss : 0.339054\n",
      "epoch 1/2 batch 97/390 g_loss : 2.776239\n",
      "epoch 1/2 batch 97/390 d_loss : 0.343530\n",
      "epoch 1/2 batch 98/390 g_loss : 2.291806\n",
      "epoch 1/2 batch 98/390 d_loss : 0.379779\n",
      "epoch 1/2 batch 99/390 g_loss : 2.297661\n",
      "epoch 1/2 batch 99/390 d_loss : 0.354507\n",
      "epoch 1/2 batch 100/390 g_loss : 2.158398\n",
      "epoch 1/2 batch 100/390 d_loss : 0.395384\n",
      "epoch 1/2 batch 101/390 g_loss : 2.213333\n",
      "epoch 1/2 batch 101/390 d_loss : 0.359649\n",
      "epoch 1/2 batch 102/390 g_loss : 3.064312\n",
      "epoch 1/2 batch 102/390 d_loss : 0.366713\n",
      "epoch 1/2 batch 103/390 g_loss : 2.548737\n",
      "epoch 1/2 batch 103/390 d_loss : 0.345480\n",
      "epoch 1/2 batch 104/390 g_loss : 2.174382\n",
      "epoch 1/2 batch 104/390 d_loss : 0.385224\n",
      "epoch 1/2 batch 105/390 g_loss : 2.944671\n",
      "epoch 1/2 batch 105/390 d_loss : 0.311115\n",
      "epoch 1/2 batch 106/390 g_loss : 2.608567\n",
      "epoch 1/2 batch 106/390 d_loss : 0.380942\n",
      "epoch 1/2 batch 107/390 g_loss : 2.204391\n",
      "epoch 1/2 batch 107/390 d_loss : 0.362486\n",
      "epoch 1/2 batch 108/390 g_loss : 2.485927\n",
      "epoch 1/2 batch 108/390 d_loss : 0.352026\n",
      "epoch 1/2 batch 109/390 g_loss : 2.359223\n",
      "epoch 1/2 batch 109/390 d_loss : 0.352867\n",
      "epoch 1/2 batch 110/390 g_loss : 2.663580\n",
      "epoch 1/2 batch 110/390 d_loss : 0.365270\n",
      "epoch 1/2 batch 111/390 g_loss : 2.311281\n",
      "epoch 1/2 batch 111/390 d_loss : 0.379130\n",
      "epoch 1/2 batch 112/390 g_loss : 3.074952\n",
      "epoch 1/2 batch 112/390 d_loss : 0.416823\n",
      "epoch 1/2 batch 113/390 g_loss : 1.672850\n",
      "epoch 1/2 batch 113/390 d_loss : 0.503694\n",
      "epoch 1/2 batch 114/390 g_loss : 2.462673\n",
      "epoch 1/2 batch 114/390 d_loss : 0.368873\n",
      "epoch 1/2 batch 115/390 g_loss : 2.130919\n",
      "epoch 1/2 batch 115/390 d_loss : 0.343565\n",
      "epoch 1/2 batch 116/390 g_loss : 2.468180\n",
      "epoch 1/2 batch 116/390 d_loss : 0.391685\n",
      "epoch 1/2 batch 117/390 g_loss : 1.833820\n",
      "epoch 1/2 batch 117/390 d_loss : 0.394823\n",
      "epoch 1/2 batch 118/390 g_loss : 2.438132\n",
      "epoch 1/2 batch 118/390 d_loss : 0.348271\n",
      "epoch 1/2 batch 119/390 g_loss : 2.365133\n",
      "epoch 1/2 batch 119/390 d_loss : 0.394623\n",
      "epoch 1/2 batch 120/390 g_loss : 1.910096\n",
      "epoch 1/2 batch 120/390 d_loss : 0.375536\n",
      "epoch 1/2 batch 121/390 g_loss : 2.415257\n",
      "epoch 1/2 batch 121/390 d_loss : 0.391855\n",
      "epoch 1/2 batch 122/390 g_loss : 2.522846\n",
      "epoch 1/2 batch 122/390 d_loss : 0.369511\n",
      "epoch 1/2 batch 123/390 g_loss : 2.202477\n",
      "epoch 1/2 batch 123/390 d_loss : 0.456646\n",
      "epoch 1/2 batch 124/390 g_loss : 2.139919\n",
      "epoch 1/2 batch 124/390 d_loss : 0.398431\n",
      "epoch 1/2 batch 125/390 g_loss : 2.512422\n",
      "epoch 1/2 batch 125/390 d_loss : 0.357989\n",
      "epoch 1/2 batch 126/390 g_loss : 2.241559\n",
      "epoch 1/2 batch 126/390 d_loss : 0.355491\n",
      "epoch 1/2 batch 127/390 g_loss : 2.645395\n",
      "epoch 1/2 batch 127/390 d_loss : 0.356674\n",
      "epoch 1/2 batch 128/390 g_loss : 2.781332\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 128/390 d_loss : 0.351641\n",
      "epoch 1/2 batch 129/390 g_loss : 1.912552\n",
      "epoch 1/2 batch 129/390 d_loss : 0.361313\n",
      "epoch 1/2 batch 130/390 g_loss : 2.807976\n",
      "epoch 1/2 batch 130/390 d_loss : 0.373234\n",
      "epoch 1/2 batch 131/390 g_loss : 2.296547\n",
      "epoch 1/2 batch 131/390 d_loss : 0.434634\n",
      "epoch 1/2 batch 132/390 g_loss : 2.728988\n",
      "epoch 1/2 batch 132/390 d_loss : 0.369514\n",
      "epoch 1/2 batch 133/390 g_loss : 1.941312\n",
      "epoch 1/2 batch 133/390 d_loss : 0.417168\n",
      "epoch 1/2 batch 134/390 g_loss : 2.824460\n",
      "epoch 1/2 batch 134/390 d_loss : 0.375127\n",
      "epoch 1/2 batch 135/390 g_loss : 2.336487\n",
      "epoch 1/2 batch 135/390 d_loss : 0.340428\n",
      "epoch 1/2 batch 136/390 g_loss : 2.518452\n",
      "epoch 1/2 batch 136/390 d_loss : 0.370008\n",
      "epoch 1/2 batch 137/390 g_loss : 2.266846\n",
      "epoch 1/2 batch 137/390 d_loss : 0.400229\n",
      "epoch 1/2 batch 138/390 g_loss : 2.426080\n",
      "epoch 1/2 batch 138/390 d_loss : 0.426838\n",
      "epoch 1/2 batch 139/390 g_loss : 1.927218\n",
      "epoch 1/2 batch 139/390 d_loss : 0.342286\n",
      "epoch 1/2 batch 140/390 g_loss : 2.837306\n",
      "epoch 1/2 batch 140/390 d_loss : 0.357995\n",
      "epoch 1/2 batch 141/390 g_loss : 2.068249\n",
      "epoch 1/2 batch 141/390 d_loss : 0.388429\n",
      "epoch 1/2 batch 142/390 g_loss : 2.560046\n",
      "epoch 1/2 batch 142/390 d_loss : 0.383171\n",
      "epoch 1/2 batch 143/390 g_loss : 2.718312\n",
      "epoch 1/2 batch 143/390 d_loss : 0.395581\n",
      "epoch 1/2 batch 144/390 g_loss : 1.631998\n",
      "epoch 1/2 batch 144/390 d_loss : 0.462218\n",
      "epoch 1/2 batch 145/390 g_loss : 2.388496\n",
      "epoch 1/2 batch 145/390 d_loss : 0.336218\n",
      "epoch 1/2 batch 146/390 g_loss : 2.181089\n",
      "epoch 1/2 batch 146/390 d_loss : 0.383171\n",
      "epoch 1/2 batch 147/390 g_loss : 2.330897\n",
      "epoch 1/2 batch 147/390 d_loss : 0.366586\n",
      "epoch 1/2 batch 148/390 g_loss : 2.292353\n",
      "epoch 1/2 batch 148/390 d_loss : 0.390855\n",
      "epoch 1/2 batch 149/390 g_loss : 2.411630\n",
      "epoch 1/2 batch 149/390 d_loss : 0.393812\n",
      "epoch 1/2 batch 150/390 g_loss : 2.334859\n",
      "epoch 1/2 batch 150/390 d_loss : 0.382020\n",
      "epoch 1/2 batch 151/390 g_loss : 2.142671\n",
      "epoch 1/2 batch 151/390 d_loss : 0.360739\n",
      "epoch 1/2 batch 152/390 g_loss : 2.542802\n",
      "epoch 1/2 batch 152/390 d_loss : 0.372316\n",
      "epoch 1/2 batch 153/390 g_loss : 1.670670\n",
      "epoch 1/2 batch 153/390 d_loss : 0.396933\n",
      "epoch 1/2 batch 154/390 g_loss : 2.805150\n",
      "epoch 1/2 batch 154/390 d_loss : 0.343703\n",
      "epoch 1/2 batch 155/390 g_loss : 2.167089\n",
      "epoch 1/2 batch 155/390 d_loss : 0.374000\n",
      "epoch 1/2 batch 156/390 g_loss : 2.196706\n",
      "epoch 1/2 batch 156/390 d_loss : 0.373626\n",
      "epoch 1/2 batch 157/390 g_loss : 2.304486\n",
      "epoch 1/2 batch 157/390 d_loss : 0.385179\n",
      "epoch 1/2 batch 158/390 g_loss : 2.310737\n",
      "epoch 1/2 batch 158/390 d_loss : 0.344980\n",
      "epoch 1/2 batch 159/390 g_loss : 2.935501\n",
      "epoch 1/2 batch 159/390 d_loss : 0.377932\n",
      "epoch 1/2 batch 160/390 g_loss : 2.226478\n",
      "epoch 1/2 batch 160/390 d_loss : 0.375971\n",
      "epoch 1/2 batch 161/390 g_loss : 2.255951\n",
      "epoch 1/2 batch 161/390 d_loss : 0.361993\n",
      "epoch 1/2 batch 162/390 g_loss : 2.997556\n",
      "epoch 1/2 batch 162/390 d_loss : 0.434433\n",
      "epoch 1/2 batch 163/390 g_loss : 1.705415\n",
      "epoch 1/2 batch 163/390 d_loss : 0.379841\n",
      "epoch 1/2 batch 164/390 g_loss : 2.464132\n",
      "epoch 1/2 batch 164/390 d_loss : 0.384291\n",
      "epoch 1/2 batch 165/390 g_loss : 2.691967\n",
      "epoch 1/2 batch 165/390 d_loss : 0.399656\n",
      "epoch 1/2 batch 166/390 g_loss : 2.198074\n",
      "epoch 1/2 batch 166/390 d_loss : 0.344425\n",
      "epoch 1/2 batch 167/390 g_loss : 1.659775\n",
      "epoch 1/2 batch 167/390 d_loss : 0.401646\n",
      "epoch 1/2 batch 168/390 g_loss : 2.913118\n",
      "epoch 1/2 batch 168/390 d_loss : 0.383510\n",
      "epoch 1/2 batch 169/390 g_loss : 2.584151\n",
      "epoch 1/2 batch 169/390 d_loss : 0.403194\n",
      "epoch 1/2 batch 170/390 g_loss : 2.603627\n",
      "epoch 1/2 batch 170/390 d_loss : 0.368328\n",
      "epoch 1/2 batch 171/390 g_loss : 2.482371\n",
      "epoch 1/2 batch 171/390 d_loss : 0.418254\n",
      "epoch 1/2 batch 172/390 g_loss : 2.638581\n",
      "epoch 1/2 batch 172/390 d_loss : 0.405757\n",
      "epoch 1/2 batch 173/390 g_loss : 2.484676\n",
      "epoch 1/2 batch 173/390 d_loss : 0.366531\n",
      "epoch 1/2 batch 174/390 g_loss : 2.552650\n",
      "epoch 1/2 batch 174/390 d_loss : 0.359106\n",
      "epoch 1/2 batch 175/390 g_loss : 2.502084\n",
      "epoch 1/2 batch 175/390 d_loss : 0.365394\n",
      "epoch 1/2 batch 176/390 g_loss : 2.457970\n",
      "epoch 1/2 batch 176/390 d_loss : 0.409694\n",
      "epoch 1/2 batch 177/390 g_loss : 3.424760\n",
      "epoch 1/2 batch 177/390 d_loss : 0.440274\n",
      "epoch 1/2 batch 178/390 g_loss : 1.608365\n",
      "epoch 1/2 batch 178/390 d_loss : 0.447075\n",
      "epoch 1/2 batch 179/390 g_loss : 2.645659\n",
      "epoch 1/2 batch 179/390 d_loss : 0.402653\n",
      "epoch 1/2 batch 180/390 g_loss : 2.654223\n",
      "epoch 1/2 batch 180/390 d_loss : 0.395659\n",
      "epoch 1/2 batch 181/390 g_loss : 2.327120\n",
      "epoch 1/2 batch 181/390 d_loss : 0.355270\n",
      "epoch 1/2 batch 182/390 g_loss : 1.693158\n",
      "epoch 1/2 batch 182/390 d_loss : 0.371692\n",
      "epoch 1/2 batch 183/390 g_loss : 2.431193\n",
      "epoch 1/2 batch 183/390 d_loss : 0.431526\n",
      "epoch 1/2 batch 184/390 g_loss : 2.377481\n",
      "epoch 1/2 batch 184/390 d_loss : 0.368761\n",
      "epoch 1/2 batch 185/390 g_loss : 2.185942\n",
      "epoch 1/2 batch 185/390 d_loss : 0.309088\n",
      "epoch 1/2 batch 186/390 g_loss : 2.549257\n",
      "epoch 1/2 batch 186/390 d_loss : 0.332470\n",
      "epoch 1/2 batch 187/390 g_loss : 2.665627\n",
      "epoch 1/2 batch 187/390 d_loss : 0.333194\n",
      "epoch 1/2 batch 188/390 g_loss : 2.102727\n",
      "epoch 1/2 batch 188/390 d_loss : 0.445813\n",
      "epoch 1/2 batch 189/390 g_loss : 2.904162\n",
      "epoch 1/2 batch 189/390 d_loss : 0.366953\n",
      "epoch 1/2 batch 190/390 g_loss : 2.000743\n",
      "epoch 1/2 batch 190/390 d_loss : 0.381035\n",
      "epoch 1/2 batch 191/390 g_loss : 3.087869\n",
      "epoch 1/2 batch 191/390 d_loss : 0.380449\n",
      "epoch 1/2 batch 192/390 g_loss : 2.332130\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 192/390 d_loss : 0.349951\n",
      "epoch 1/2 batch 193/390 g_loss : 2.763759\n",
      "epoch 1/2 batch 193/390 d_loss : 0.405841\n",
      "epoch 1/2 batch 194/390 g_loss : 2.104996\n",
      "epoch 1/2 batch 194/390 d_loss : 0.406659\n",
      "epoch 1/2 batch 195/390 g_loss : 2.759822\n",
      "epoch 1/2 batch 195/390 d_loss : 0.375243\n",
      "epoch 1/2 batch 196/390 g_loss : 3.521040\n",
      "epoch 1/2 batch 196/390 d_loss : 0.421782\n",
      "epoch 1/2 batch 197/390 g_loss : 2.067331\n",
      "epoch 1/2 batch 197/390 d_loss : 0.355510\n",
      "epoch 1/2 batch 198/390 g_loss : 2.994519\n",
      "epoch 1/2 batch 198/390 d_loss : 0.331720\n",
      "epoch 1/2 batch 199/390 g_loss : 3.293077\n",
      "epoch 1/2 batch 199/390 d_loss : 0.390117\n",
      "epoch 1/2 batch 200/390 g_loss : 2.499107\n",
      "epoch 1/2 batch 200/390 d_loss : 0.372932\n",
      "epoch 1/2 batch 201/390 g_loss : 3.433806\n",
      "epoch 1/2 batch 201/390 d_loss : 0.359366\n",
      "epoch 1/2 batch 202/390 g_loss : 2.207545\n",
      "epoch 1/2 batch 202/390 d_loss : 0.405390\n",
      "epoch 1/2 batch 203/390 g_loss : 2.771829\n",
      "epoch 1/2 batch 203/390 d_loss : 0.325151\n",
      "epoch 1/2 batch 204/390 g_loss : 2.461603\n",
      "epoch 1/2 batch 204/390 d_loss : 0.308252\n",
      "epoch 1/2 batch 205/390 g_loss : 2.484920\n",
      "epoch 1/2 batch 205/390 d_loss : 0.374480\n",
      "epoch 1/2 batch 206/390 g_loss : 3.284494\n",
      "epoch 1/2 batch 206/390 d_loss : 0.383985\n",
      "epoch 1/2 batch 207/390 g_loss : 2.470915\n",
      "epoch 1/2 batch 207/390 d_loss : 0.362577\n",
      "epoch 1/2 batch 208/390 g_loss : 3.260895\n",
      "epoch 1/2 batch 208/390 d_loss : 0.348618\n",
      "epoch 1/2 batch 209/390 g_loss : 2.083871\n",
      "epoch 1/2 batch 209/390 d_loss : 0.391188\n",
      "epoch 1/2 batch 210/390 g_loss : 2.142151\n",
      "epoch 1/2 batch 210/390 d_loss : 0.378281\n",
      "epoch 1/2 batch 211/390 g_loss : 3.485634\n",
      "epoch 1/2 batch 211/390 d_loss : 0.395010\n",
      "epoch 1/2 batch 212/390 g_loss : 1.652661\n",
      "epoch 1/2 batch 212/390 d_loss : 0.407637\n",
      "epoch 1/2 batch 213/390 g_loss : 3.168259\n",
      "epoch 1/2 batch 213/390 d_loss : 0.474690\n",
      "epoch 1/2 batch 214/390 g_loss : 2.136850\n",
      "epoch 1/2 batch 214/390 d_loss : 0.424692\n",
      "epoch 1/2 batch 215/390 g_loss : 2.957490\n",
      "epoch 1/2 batch 215/390 d_loss : 0.388104\n",
      "epoch 1/2 batch 216/390 g_loss : 2.963861\n",
      "epoch 1/2 batch 216/390 d_loss : 0.475422\n",
      "epoch 1/2 batch 217/390 g_loss : 1.818349\n",
      "epoch 1/2 batch 217/390 d_loss : 0.429950\n",
      "epoch 1/2 batch 218/390 g_loss : 2.653799\n",
      "epoch 1/2 batch 218/390 d_loss : 0.353613\n",
      "epoch 1/2 batch 219/390 g_loss : 2.889827\n",
      "epoch 1/2 batch 219/390 d_loss : 0.408831\n",
      "epoch 1/2 batch 220/390 g_loss : 2.878232\n",
      "epoch 1/2 batch 220/390 d_loss : 0.396066\n",
      "epoch 1/2 batch 221/390 g_loss : 2.388334\n",
      "epoch 1/2 batch 221/390 d_loss : 0.364187\n",
      "epoch 1/2 batch 222/390 g_loss : 2.433551\n",
      "epoch 1/2 batch 222/390 d_loss : 0.401227\n",
      "epoch 1/2 batch 223/390 g_loss : 1.974611\n",
      "epoch 1/2 batch 223/390 d_loss : 0.415074\n",
      "epoch 1/2 batch 224/390 g_loss : 2.405669\n",
      "epoch 1/2 batch 224/390 d_loss : 0.359453\n",
      "epoch 1/2 batch 225/390 g_loss : 2.342110\n",
      "epoch 1/2 batch 225/390 d_loss : 0.313562\n",
      "epoch 1/2 batch 226/390 g_loss : 2.926265\n",
      "epoch 1/2 batch 226/390 d_loss : 0.359426\n",
      "epoch 1/2 batch 227/390 g_loss : 2.012275\n",
      "epoch 1/2 batch 227/390 d_loss : 0.380247\n",
      "epoch 1/2 batch 228/390 g_loss : 3.055374\n",
      "epoch 1/2 batch 228/390 d_loss : 0.398493\n",
      "epoch 1/2 batch 229/390 g_loss : 2.258465\n",
      "epoch 1/2 batch 229/390 d_loss : 0.453865\n",
      "epoch 1/2 batch 230/390 g_loss : 3.011242\n",
      "epoch 1/2 batch 230/390 d_loss : 0.434687\n",
      "epoch 1/2 batch 231/390 g_loss : 2.243582\n",
      "epoch 1/2 batch 231/390 d_loss : 0.405708\n",
      "epoch 1/2 batch 232/390 g_loss : 2.912072\n",
      "epoch 1/2 batch 232/390 d_loss : 0.375903\n",
      "epoch 1/2 batch 233/390 g_loss : 2.921493\n",
      "epoch 1/2 batch 233/390 d_loss : 0.404175\n",
      "epoch 1/2 batch 234/390 g_loss : 2.973667\n",
      "epoch 1/2 batch 234/390 d_loss : 0.318598\n",
      "epoch 1/2 batch 235/390 g_loss : 2.935318\n",
      "epoch 1/2 batch 235/390 d_loss : 0.357794\n",
      "epoch 1/2 batch 236/390 g_loss : 2.327647\n",
      "epoch 1/2 batch 236/390 d_loss : 0.435855\n",
      "epoch 1/2 batch 237/390 g_loss : 2.738780\n",
      "epoch 1/2 batch 237/390 d_loss : 0.359874\n",
      "epoch 1/2 batch 238/390 g_loss : 2.202514\n",
      "epoch 1/2 batch 238/390 d_loss : 0.371891\n",
      "epoch 1/2 batch 239/390 g_loss : 2.111826\n",
      "epoch 1/2 batch 239/390 d_loss : 0.362120\n",
      "epoch 1/2 batch 240/390 g_loss : 2.657538\n",
      "epoch 1/2 batch 240/390 d_loss : 0.331689\n",
      "epoch 1/2 batch 241/390 g_loss : 2.295387\n",
      "epoch 1/2 batch 241/390 d_loss : 0.332462\n",
      "epoch 1/2 batch 242/390 g_loss : 2.397603\n",
      "epoch 1/2 batch 242/390 d_loss : 0.349335\n",
      "epoch 1/2 batch 243/390 g_loss : 2.878998\n",
      "epoch 1/2 batch 243/390 d_loss : 0.380421\n",
      "epoch 1/2 batch 244/390 g_loss : 2.658340\n",
      "epoch 1/2 batch 244/390 d_loss : 0.370744\n",
      "epoch 1/2 batch 245/390 g_loss : 2.338096\n",
      "epoch 1/2 batch 245/390 d_loss : 0.349881\n",
      "epoch 1/2 batch 246/390 g_loss : 2.934075\n",
      "epoch 1/2 batch 246/390 d_loss : 0.329868\n",
      "epoch 1/2 batch 247/390 g_loss : 3.283207\n",
      "epoch 1/2 batch 247/390 d_loss : 0.364354\n",
      "epoch 1/2 batch 248/390 g_loss : 2.345175\n",
      "epoch 1/2 batch 248/390 d_loss : 0.353454\n",
      "epoch 1/2 batch 249/390 g_loss : 3.474814\n",
      "epoch 1/2 batch 249/390 d_loss : 0.396984\n",
      "epoch 1/2 batch 250/390 g_loss : 2.246615\n",
      "epoch 1/2 batch 250/390 d_loss : 0.427306\n",
      "epoch 1/2 batch 251/390 g_loss : 2.775299\n",
      "epoch 1/2 batch 251/390 d_loss : 0.427107\n",
      "epoch 1/2 batch 252/390 g_loss : 2.233472\n",
      "epoch 1/2 batch 252/390 d_loss : 0.465297\n",
      "epoch 1/2 batch 253/390 g_loss : 3.138888\n",
      "epoch 1/2 batch 253/390 d_loss : 0.445424\n",
      "epoch 1/2 batch 254/390 g_loss : 1.671454\n",
      "epoch 1/2 batch 254/390 d_loss : 0.451447\n",
      "epoch 1/2 batch 255/390 g_loss : 3.133305\n",
      "epoch 1/2 batch 255/390 d_loss : 0.462640\n",
      "epoch 1/2 batch 256/390 g_loss : 1.993121\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 256/390 d_loss : 0.463235\n",
      "epoch 1/2 batch 257/390 g_loss : 2.165436\n",
      "epoch 1/2 batch 257/390 d_loss : 0.456722\n",
      "epoch 1/2 batch 258/390 g_loss : 2.401506\n",
      "epoch 1/2 batch 258/390 d_loss : 0.412852\n",
      "epoch 1/2 batch 259/390 g_loss : 2.142151\n",
      "epoch 1/2 batch 259/390 d_loss : 0.393686\n",
      "epoch 1/2 batch 260/390 g_loss : 2.272331\n",
      "epoch 1/2 batch 260/390 d_loss : 0.374969\n",
      "epoch 1/2 batch 261/390 g_loss : 3.127357\n",
      "epoch 1/2 batch 261/390 d_loss : 0.372926\n",
      "epoch 1/2 batch 262/390 g_loss : 2.143097\n",
      "epoch 1/2 batch 262/390 d_loss : 0.392012\n",
      "epoch 1/2 batch 263/390 g_loss : 2.220041\n",
      "epoch 1/2 batch 263/390 d_loss : 0.338119\n",
      "epoch 1/2 batch 264/390 g_loss : 3.108615\n",
      "epoch 1/2 batch 264/390 d_loss : 0.416784\n",
      "epoch 1/2 batch 265/390 g_loss : 1.998950\n",
      "epoch 1/2 batch 265/390 d_loss : 0.376865\n",
      "epoch 1/2 batch 266/390 g_loss : 2.839372\n",
      "epoch 1/2 batch 266/390 d_loss : 0.329695\n",
      "epoch 1/2 batch 267/390 g_loss : 3.112272\n",
      "epoch 1/2 batch 267/390 d_loss : 0.379372\n",
      "epoch 1/2 batch 268/390 g_loss : 1.610548\n",
      "epoch 1/2 batch 268/390 d_loss : 0.471598\n",
      "epoch 1/2 batch 269/390 g_loss : 2.846539\n",
      "epoch 1/2 batch 269/390 d_loss : 0.398257\n",
      "epoch 1/2 batch 270/390 g_loss : 1.676117\n",
      "epoch 1/2 batch 270/390 d_loss : 0.541467\n",
      "epoch 1/2 batch 271/390 g_loss : 4.043457\n",
      "epoch 1/2 batch 271/390 d_loss : 0.642607\n",
      "epoch 1/2 batch 272/390 g_loss : 1.098747\n",
      "epoch 1/2 batch 272/390 d_loss : 0.622228\n",
      "epoch 1/2 batch 273/390 g_loss : 2.347036\n",
      "epoch 1/2 batch 273/390 d_loss : 0.435148\n",
      "epoch 1/2 batch 274/390 g_loss : 2.461414\n",
      "epoch 1/2 batch 274/390 d_loss : 0.438094\n",
      "epoch 1/2 batch 275/390 g_loss : 2.574584\n",
      "epoch 1/2 batch 275/390 d_loss : 0.436644\n",
      "epoch 1/2 batch 276/390 g_loss : 1.566973\n",
      "epoch 1/2 batch 276/390 d_loss : 0.513786\n",
      "epoch 1/2 batch 277/390 g_loss : 5.137679\n",
      "epoch 1/2 batch 277/390 d_loss : 0.814956\n",
      "epoch 1/2 batch 278/390 g_loss : 1.192481\n",
      "epoch 1/2 batch 278/390 d_loss : 0.572450\n",
      "epoch 1/2 batch 279/390 g_loss : 1.686683\n",
      "epoch 1/2 batch 279/390 d_loss : 0.480751\n",
      "epoch 1/2 batch 280/390 g_loss : 3.106673\n",
      "epoch 1/2 batch 280/390 d_loss : 0.563952\n",
      "epoch 1/2 batch 281/390 g_loss : 1.776993\n",
      "epoch 1/2 batch 281/390 d_loss : 0.473362\n",
      "epoch 1/2 batch 282/390 g_loss : 2.244920\n",
      "epoch 1/2 batch 282/390 d_loss : 0.441890\n",
      "epoch 1/2 batch 283/390 g_loss : 2.288172\n",
      "epoch 1/2 batch 283/390 d_loss : 0.394829\n",
      "epoch 1/2 batch 284/390 g_loss : 2.328721\n",
      "epoch 1/2 batch 284/390 d_loss : 0.402492\n",
      "epoch 1/2 batch 285/390 g_loss : 2.591658\n",
      "epoch 1/2 batch 285/390 d_loss : 0.389193\n",
      "epoch 1/2 batch 286/390 g_loss : 2.203431\n",
      "epoch 1/2 batch 286/390 d_loss : 0.371731\n",
      "epoch 1/2 batch 287/390 g_loss : 2.294079\n",
      "epoch 1/2 batch 287/390 d_loss : 0.361800\n",
      "epoch 1/2 batch 288/390 g_loss : 2.693213\n",
      "epoch 1/2 batch 288/390 d_loss : 0.392741\n",
      "epoch 1/2 batch 289/390 g_loss : 2.233964\n",
      "epoch 1/2 batch 289/390 d_loss : 0.372006\n",
      "epoch 1/2 batch 290/390 g_loss : 1.806605\n",
      "epoch 1/2 batch 290/390 d_loss : 0.454522\n",
      "epoch 1/2 batch 291/390 g_loss : 2.490200\n",
      "epoch 1/2 batch 291/390 d_loss : 0.404877\n",
      "epoch 1/2 batch 292/390 g_loss : 2.532151\n",
      "epoch 1/2 batch 292/390 d_loss : 0.318831\n",
      "epoch 1/2 batch 293/390 g_loss : 2.389747\n",
      "epoch 1/2 batch 293/390 d_loss : 0.331486\n",
      "epoch 1/2 batch 294/390 g_loss : 2.425275\n",
      "epoch 1/2 batch 294/390 d_loss : 0.375779\n",
      "epoch 1/2 batch 295/390 g_loss : 2.483667\n",
      "epoch 1/2 batch 295/390 d_loss : 0.413369\n",
      "epoch 1/2 batch 296/390 g_loss : 1.883782\n",
      "epoch 1/2 batch 296/390 d_loss : 0.416585\n",
      "epoch 1/2 batch 297/390 g_loss : 2.381264\n",
      "epoch 1/2 batch 297/390 d_loss : 0.366510\n",
      "epoch 1/2 batch 298/390 g_loss : 2.565951\n",
      "epoch 1/2 batch 298/390 d_loss : 0.320799\n",
      "epoch 1/2 batch 299/390 g_loss : 2.809520\n",
      "epoch 1/2 batch 299/390 d_loss : 0.348228\n",
      "epoch 1/2 batch 300/390 g_loss : 2.694056\n",
      "epoch 1/2 batch 300/390 d_loss : 0.332272\n",
      "epoch 1/2 batch 301/390 g_loss : 2.592035\n",
      "epoch 1/2 batch 301/390 d_loss : 0.370939\n",
      "epoch 1/2 batch 302/390 g_loss : 2.511449\n",
      "epoch 1/2 batch 302/390 d_loss : 0.295103\n",
      "epoch 1/2 batch 303/390 g_loss : 2.610482\n",
      "epoch 1/2 batch 303/390 d_loss : 0.465442\n",
      "epoch 1/2 batch 304/390 g_loss : 3.077966\n",
      "epoch 1/2 batch 304/390 d_loss : 0.438942\n",
      "epoch 1/2 batch 305/390 g_loss : 2.177706\n",
      "epoch 1/2 batch 305/390 d_loss : 0.339879\n",
      "epoch 1/2 batch 306/390 g_loss : 2.326000\n",
      "epoch 1/2 batch 306/390 d_loss : 0.354470\n",
      "epoch 1/2 batch 307/390 g_loss : 2.369464\n",
      "epoch 1/2 batch 307/390 d_loss : 0.349325\n",
      "epoch 1/2 batch 308/390 g_loss : 2.536597\n",
      "epoch 1/2 batch 308/390 d_loss : 0.317793\n",
      "epoch 1/2 batch 309/390 g_loss : 2.520110\n",
      "epoch 1/2 batch 309/390 d_loss : 0.344946\n",
      "epoch 1/2 batch 310/390 g_loss : 2.579814\n",
      "epoch 1/2 batch 310/390 d_loss : 0.309823\n",
      "epoch 1/2 batch 311/390 g_loss : 2.670109\n",
      "epoch 1/2 batch 311/390 d_loss : 0.377709\n",
      "epoch 1/2 batch 312/390 g_loss : 2.277453\n",
      "epoch 1/2 batch 312/390 d_loss : 0.340369\n",
      "epoch 1/2 batch 313/390 g_loss : 1.977055\n",
      "epoch 1/2 batch 313/390 d_loss : 0.433317\n",
      "epoch 1/2 batch 314/390 g_loss : 2.762518\n",
      "epoch 1/2 batch 314/390 d_loss : 0.357067\n",
      "epoch 1/2 batch 315/390 g_loss : 2.139187\n",
      "epoch 1/2 batch 315/390 d_loss : 0.411968\n",
      "epoch 1/2 batch 316/390 g_loss : 2.212009\n",
      "epoch 1/2 batch 316/390 d_loss : 0.404206\n",
      "epoch 1/2 batch 317/390 g_loss : 1.746781\n",
      "epoch 1/2 batch 317/390 d_loss : 0.369333\n",
      "epoch 1/2 batch 318/390 g_loss : 2.722658\n",
      "epoch 1/2 batch 318/390 d_loss : 0.323549\n",
      "epoch 1/2 batch 319/390 g_loss : 2.663277\n",
      "epoch 1/2 batch 319/390 d_loss : 0.330072\n",
      "epoch 1/2 batch 320/390 g_loss : 2.156467\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 320/390 d_loss : 0.426100\n",
      "epoch 1/2 batch 321/390 g_loss : 2.646919\n",
      "epoch 1/2 batch 321/390 d_loss : 0.385266\n",
      "epoch 1/2 batch 322/390 g_loss : 2.246083\n",
      "epoch 1/2 batch 322/390 d_loss : 0.445567\n",
      "epoch 1/2 batch 323/390 g_loss : 2.231351\n",
      "epoch 1/2 batch 323/390 d_loss : 0.392202\n",
      "epoch 1/2 batch 324/390 g_loss : 2.706401\n",
      "epoch 1/2 batch 324/390 d_loss : 0.378897\n",
      "epoch 1/2 batch 325/390 g_loss : 2.375568\n",
      "epoch 1/2 batch 325/390 d_loss : 0.366291\n",
      "epoch 1/2 batch 326/390 g_loss : 2.111273\n",
      "epoch 1/2 batch 326/390 d_loss : 0.394987\n",
      "epoch 1/2 batch 327/390 g_loss : 2.912872\n",
      "epoch 1/2 batch 327/390 d_loss : 0.342244\n",
      "epoch 1/2 batch 328/390 g_loss : 2.890992\n",
      "epoch 1/2 batch 328/390 d_loss : 0.403188\n",
      "epoch 1/2 batch 329/390 g_loss : 2.079740\n",
      "epoch 1/2 batch 329/390 d_loss : 0.338119\n",
      "epoch 1/2 batch 330/390 g_loss : 2.412740\n",
      "epoch 1/2 batch 330/390 d_loss : 0.367283\n",
      "epoch 1/2 batch 331/390 g_loss : 2.638496\n",
      "epoch 1/2 batch 331/390 d_loss : 0.367615\n",
      "epoch 1/2 batch 332/390 g_loss : 2.412594\n",
      "epoch 1/2 batch 332/390 d_loss : 0.378986\n",
      "epoch 1/2 batch 333/390 g_loss : 1.901558\n",
      "epoch 1/2 batch 333/390 d_loss : 0.378513\n",
      "epoch 1/2 batch 334/390 g_loss : 2.983375\n",
      "epoch 1/2 batch 334/390 d_loss : 0.388057\n",
      "epoch 1/2 batch 335/390 g_loss : 2.309915\n",
      "epoch 1/2 batch 335/390 d_loss : 0.329874\n",
      "epoch 1/2 batch 336/390 g_loss : 2.186604\n",
      "epoch 1/2 batch 336/390 d_loss : 0.388814\n",
      "epoch 1/2 batch 337/390 g_loss : 2.328032\n",
      "epoch 1/2 batch 337/390 d_loss : 0.334254\n",
      "epoch 1/2 batch 338/390 g_loss : 2.168172\n",
      "epoch 1/2 batch 338/390 d_loss : 0.334019\n",
      "epoch 1/2 batch 339/390 g_loss : 2.544152\n",
      "epoch 1/2 batch 339/390 d_loss : 0.370692\n",
      "epoch 1/2 batch 340/390 g_loss : 2.527583\n",
      "epoch 1/2 batch 340/390 d_loss : 0.329325\n",
      "epoch 1/2 batch 341/390 g_loss : 2.829161\n",
      "epoch 1/2 batch 341/390 d_loss : 0.407528\n",
      "epoch 1/2 batch 342/390 g_loss : 1.660741\n",
      "epoch 1/2 batch 342/390 d_loss : 0.453068\n",
      "epoch 1/2 batch 343/390 g_loss : 2.985612\n",
      "epoch 1/2 batch 343/390 d_loss : 0.393586\n",
      "epoch 1/2 batch 344/390 g_loss : 1.810341\n",
      "epoch 1/2 batch 344/390 d_loss : 0.392998\n",
      "epoch 1/2 batch 345/390 g_loss : 2.782109\n",
      "epoch 1/2 batch 345/390 d_loss : 0.382548\n",
      "epoch 1/2 batch 346/390 g_loss : 1.995716\n",
      "epoch 1/2 batch 346/390 d_loss : 0.378160\n",
      "epoch 1/2 batch 347/390 g_loss : 2.614154\n",
      "epoch 1/2 batch 347/390 d_loss : 0.384268\n",
      "epoch 1/2 batch 348/390 g_loss : 2.186379\n",
      "epoch 1/2 batch 348/390 d_loss : 0.423023\n",
      "epoch 1/2 batch 349/390 g_loss : 2.201741\n",
      "epoch 1/2 batch 349/390 d_loss : 0.369804\n",
      "epoch 1/2 batch 350/390 g_loss : 2.664550\n",
      "epoch 1/2 batch 350/390 d_loss : 0.339711\n",
      "epoch 1/2 batch 351/390 g_loss : 2.559295\n",
      "epoch 1/2 batch 351/390 d_loss : 0.363316\n",
      "epoch 1/2 batch 352/390 g_loss : 2.573913\n",
      "epoch 1/2 batch 352/390 d_loss : 0.351783\n",
      "epoch 1/2 batch 353/390 g_loss : 2.814763\n",
      "epoch 1/2 batch 353/390 d_loss : 0.411711\n",
      "epoch 1/2 batch 354/390 g_loss : 2.407255\n",
      "epoch 1/2 batch 354/390 d_loss : 0.364550\n",
      "epoch 1/2 batch 355/390 g_loss : 2.031194\n",
      "epoch 1/2 batch 355/390 d_loss : 0.322231\n",
      "epoch 1/2 batch 356/390 g_loss : 2.322999\n",
      "epoch 1/2 batch 356/390 d_loss : 0.332339\n",
      "epoch 1/2 batch 357/390 g_loss : 3.227363\n",
      "epoch 1/2 batch 357/390 d_loss : 0.332522\n",
      "epoch 1/2 batch 358/390 g_loss : 2.822574\n",
      "epoch 1/2 batch 358/390 d_loss : 0.345633\n",
      "epoch 1/2 batch 359/390 g_loss : 3.032076\n",
      "epoch 1/2 batch 359/390 d_loss : 0.356661\n",
      "epoch 1/2 batch 360/390 g_loss : 2.038813\n",
      "epoch 1/2 batch 360/390 d_loss : 0.477735\n",
      "epoch 1/2 batch 361/390 g_loss : 3.107043\n",
      "epoch 1/2 batch 361/390 d_loss : 0.476929\n",
      "epoch 1/2 batch 362/390 g_loss : 1.054854\n",
      "epoch 1/2 batch 362/390 d_loss : 0.599105\n",
      "epoch 1/2 batch 363/390 g_loss : 3.304688\n",
      "epoch 1/2 batch 363/390 d_loss : 0.608590\n",
      "epoch 1/2 batch 364/390 g_loss : 1.109044\n",
      "epoch 1/2 batch 364/390 d_loss : 0.598599\n",
      "epoch 1/2 batch 365/390 g_loss : 3.428684\n",
      "epoch 1/2 batch 365/390 d_loss : 0.475756\n",
      "epoch 1/2 batch 366/390 g_loss : 1.157539\n",
      "epoch 1/2 batch 366/390 d_loss : 0.583066\n",
      "epoch 1/2 batch 367/390 g_loss : 2.688467\n",
      "epoch 1/2 batch 367/390 d_loss : 0.394579\n",
      "epoch 1/2 batch 368/390 g_loss : 2.428540\n",
      "epoch 1/2 batch 368/390 d_loss : 0.461907\n",
      "epoch 1/2 batch 369/390 g_loss : 2.042661\n",
      "epoch 1/2 batch 369/390 d_loss : 0.459495\n",
      "epoch 1/2 batch 370/390 g_loss : 2.541937\n",
      "epoch 1/2 batch 370/390 d_loss : 0.467425\n",
      "epoch 1/2 batch 371/390 g_loss : 2.418380\n",
      "epoch 1/2 batch 371/390 d_loss : 0.381720\n",
      "epoch 1/2 batch 372/390 g_loss : 2.499391\n",
      "epoch 1/2 batch 372/390 d_loss : 0.472750\n",
      "epoch 1/2 batch 373/390 g_loss : 2.360088\n",
      "epoch 1/2 batch 373/390 d_loss : 0.377684\n",
      "epoch 1/2 batch 374/390 g_loss : 2.354293\n",
      "epoch 1/2 batch 374/390 d_loss : 0.356695\n",
      "epoch 1/2 batch 375/390 g_loss : 2.550903\n",
      "epoch 1/2 batch 375/390 d_loss : 0.388567\n",
      "epoch 1/2 batch 376/390 g_loss : 2.328540\n",
      "epoch 1/2 batch 376/390 d_loss : 0.359660\n",
      "epoch 1/2 batch 377/390 g_loss : 2.452537\n",
      "epoch 1/2 batch 377/390 d_loss : 0.362036\n",
      "epoch 1/2 batch 378/390 g_loss : 2.617462\n",
      "epoch 1/2 batch 378/390 d_loss : 0.348092\n",
      "epoch 1/2 batch 379/390 g_loss : 2.135314\n",
      "epoch 1/2 batch 379/390 d_loss : 0.420652\n",
      "epoch 1/2 batch 380/390 g_loss : 2.761337\n",
      "epoch 1/2 batch 380/390 d_loss : 0.422568\n",
      "epoch 1/2 batch 381/390 g_loss : 1.929425\n",
      "epoch 1/2 batch 381/390 d_loss : 0.462501\n",
      "epoch 1/2 batch 382/390 g_loss : 1.980828\n",
      "epoch 1/2 batch 382/390 d_loss : 0.386807\n",
      "epoch 1/2 batch 383/390 g_loss : 2.797242\n",
      "epoch 1/2 batch 383/390 d_loss : 0.382686\n",
      "epoch 1/2 batch 384/390 g_loss : 2.270559\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 1/2 batch 384/390 d_loss : 0.403136\n",
      "epoch 1/2 batch 385/390 g_loss : 2.985407\n",
      "epoch 1/2 batch 385/390 d_loss : 0.390900\n",
      "epoch 1/2 batch 386/390 g_loss : 2.641861\n",
      "epoch 1/2 batch 386/390 d_loss : 0.444380\n",
      "epoch 1/2 batch 387/390 g_loss : 2.245988\n",
      "epoch 1/2 batch 387/390 d_loss : 0.473558\n",
      "epoch 1/2 batch 388/390 g_loss : 2.469774\n",
      "epoch 1/2 batch 388/390 d_loss : 0.418899\n",
      "epoch 1/2 batch 389/390 g_loss : 2.092540\n",
      "epoch 1/2 batch 389/390 d_loss : 0.321351\n",
      "Epoch is 1\n",
      "Number of batches 390\n",
      "epoch 2/2 batch 0/390 g_loss : 2.261504\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 0/390 d_loss : 0.331985\n",
      "epoch 2/2 batch 1/390 g_loss : 3.089868\n",
      "epoch 2/2 batch 1/390 d_loss : 0.318956\n",
      "epoch 2/2 batch 2/390 g_loss : 1.969686\n",
      "epoch 2/2 batch 2/390 d_loss : 0.353994\n",
      "epoch 2/2 batch 3/390 g_loss : 2.876208\n",
      "epoch 2/2 batch 3/390 d_loss : 0.370644\n",
      "epoch 2/2 batch 4/390 g_loss : 2.849354\n",
      "epoch 2/2 batch 4/390 d_loss : 0.415029\n",
      "epoch 2/2 batch 5/390 g_loss : 1.847351\n",
      "epoch 2/2 batch 5/390 d_loss : 0.410324\n",
      "epoch 2/2 batch 6/390 g_loss : 2.789364\n",
      "epoch 2/2 batch 6/390 d_loss : 0.386574\n",
      "epoch 2/2 batch 7/390 g_loss : 2.774816\n",
      "epoch 2/2 batch 7/390 d_loss : 0.406162\n",
      "epoch 2/2 batch 8/390 g_loss : 2.294643\n",
      "epoch 2/2 batch 8/390 d_loss : 0.396475\n",
      "epoch 2/2 batch 9/390 g_loss : 2.700806\n",
      "epoch 2/2 batch 9/390 d_loss : 0.363120\n",
      "epoch 2/2 batch 10/390 g_loss : 2.708601\n",
      "epoch 2/2 batch 10/390 d_loss : 0.403328\n",
      "epoch 2/2 batch 11/390 g_loss : 2.733848\n",
      "epoch 2/2 batch 11/390 d_loss : 0.358913\n",
      "epoch 2/2 batch 12/390 g_loss : 2.267034\n",
      "epoch 2/2 batch 12/390 d_loss : 0.401861\n",
      "epoch 2/2 batch 13/390 g_loss : 3.095245\n",
      "epoch 2/2 batch 13/390 d_loss : 0.418429\n",
      "epoch 2/2 batch 14/390 g_loss : 2.890469\n",
      "epoch 2/2 batch 14/390 d_loss : 0.359925\n",
      "epoch 2/2 batch 15/390 g_loss : 2.204562\n",
      "epoch 2/2 batch 15/390 d_loss : 0.383412\n",
      "epoch 2/2 batch 16/390 g_loss : 2.667521\n",
      "epoch 2/2 batch 16/390 d_loss : 0.376775\n",
      "epoch 2/2 batch 17/390 g_loss : 2.511656\n",
      "epoch 2/2 batch 17/390 d_loss : 0.382979\n",
      "epoch 2/2 batch 18/390 g_loss : 2.079697\n",
      "epoch 2/2 batch 18/390 d_loss : 0.369838\n",
      "epoch 2/2 batch 19/390 g_loss : 3.120414\n",
      "epoch 2/2 batch 19/390 d_loss : 0.338117\n",
      "epoch 2/2 batch 20/390 g_loss : 2.452666\n",
      "epoch 2/2 batch 20/390 d_loss : 0.450392\n",
      "epoch 2/2 batch 21/390 g_loss : 2.825999\n",
      "epoch 2/2 batch 21/390 d_loss : 0.353224\n",
      "epoch 2/2 batch 22/390 g_loss : 2.017883\n",
      "epoch 2/2 batch 22/390 d_loss : 0.522735\n",
      "epoch 2/2 batch 23/390 g_loss : 3.075227\n",
      "epoch 2/2 batch 23/390 d_loss : 0.456096\n",
      "epoch 2/2 batch 24/390 g_loss : 1.067298\n",
      "epoch 2/2 batch 24/390 d_loss : 0.790587\n",
      "epoch 2/2 batch 25/390 g_loss : 4.253339\n",
      "epoch 2/2 batch 25/390 d_loss : 0.646576\n",
      "epoch 2/2 batch 26/390 g_loss : 1.562569\n",
      "epoch 2/2 batch 26/390 d_loss : 0.495049\n",
      "epoch 2/2 batch 27/390 g_loss : 1.531317\n",
      "epoch 2/2 batch 27/390 d_loss : 0.520761\n",
      "epoch 2/2 batch 28/390 g_loss : 2.992609\n",
      "epoch 2/2 batch 28/390 d_loss : 0.466613\n",
      "epoch 2/2 batch 29/390 g_loss : 1.865515\n",
      "epoch 2/2 batch 29/390 d_loss : 0.440045\n",
      "epoch 2/2 batch 30/390 g_loss : 2.285386\n",
      "epoch 2/2 batch 30/390 d_loss : 0.385639\n",
      "epoch 2/2 batch 31/390 g_loss : 2.619031\n",
      "epoch 2/2 batch 31/390 d_loss : 0.414362\n",
      "epoch 2/2 batch 32/390 g_loss : 2.035431\n",
      "epoch 2/2 batch 32/390 d_loss : 0.392515\n",
      "epoch 2/2 batch 33/390 g_loss : 2.453154\n",
      "epoch 2/2 batch 33/390 d_loss : 0.397786\n",
      "epoch 2/2 batch 34/390 g_loss : 2.980439\n",
      "epoch 2/2 batch 34/390 d_loss : 0.406368\n",
      "epoch 2/2 batch 35/390 g_loss : 1.746598\n",
      "epoch 2/2 batch 35/390 d_loss : 0.434650\n",
      "epoch 2/2 batch 36/390 g_loss : 2.502773\n",
      "epoch 2/2 batch 36/390 d_loss : 0.380553\n",
      "epoch 2/2 batch 37/390 g_loss : 2.557610\n",
      "epoch 2/2 batch 37/390 d_loss : 0.379216\n",
      "epoch 2/2 batch 38/390 g_loss : 2.069640\n",
      "epoch 2/2 batch 38/390 d_loss : 0.422420\n",
      "epoch 2/2 batch 39/390 g_loss : 1.936915\n",
      "epoch 2/2 batch 39/390 d_loss : 0.380315\n",
      "epoch 2/2 batch 40/390 g_loss : 2.529661\n",
      "epoch 2/2 batch 40/390 d_loss : 0.384487\n",
      "epoch 2/2 batch 41/390 g_loss : 2.114679\n",
      "epoch 2/2 batch 41/390 d_loss : 0.376379\n",
      "epoch 2/2 batch 42/390 g_loss : 2.424438\n",
      "epoch 2/2 batch 42/390 d_loss : 0.344423\n",
      "epoch 2/2 batch 43/390 g_loss : 2.082564\n",
      "epoch 2/2 batch 43/390 d_loss : 0.360974\n",
      "epoch 2/2 batch 44/390 g_loss : 2.063198\n",
      "epoch 2/2 batch 44/390 d_loss : 0.419394\n",
      "epoch 2/2 batch 45/390 g_loss : 2.166625\n",
      "epoch 2/2 batch 45/390 d_loss : 0.357217\n",
      "epoch 2/2 batch 46/390 g_loss : 2.072230\n",
      "epoch 2/2 batch 46/390 d_loss : 0.352390\n",
      "epoch 2/2 batch 47/390 g_loss : 2.458730\n",
      "epoch 2/2 batch 47/390 d_loss : 0.382800\n",
      "epoch 2/2 batch 48/390 g_loss : 2.182707\n",
      "epoch 2/2 batch 48/390 d_loss : 0.390667\n",
      "epoch 2/2 batch 49/390 g_loss : 2.097336\n",
      "epoch 2/2 batch 49/390 d_loss : 0.390746\n",
      "epoch 2/2 batch 50/390 g_loss : 2.271859\n",
      "epoch 2/2 batch 50/390 d_loss : 0.344542\n",
      "epoch 2/2 batch 51/390 g_loss : 2.332437\n",
      "epoch 2/2 batch 51/390 d_loss : 0.392581\n",
      "epoch 2/2 batch 52/390 g_loss : 2.131128\n",
      "epoch 2/2 batch 52/390 d_loss : 0.361927\n",
      "epoch 2/2 batch 53/390 g_loss : 2.115685\n",
      "epoch 2/2 batch 53/390 d_loss : 0.304611\n",
      "epoch 2/2 batch 54/390 g_loss : 2.596073\n",
      "epoch 2/2 batch 54/390 d_loss : 0.314550\n",
      "epoch 2/2 batch 55/390 g_loss : 2.196638\n",
      "epoch 2/2 batch 55/390 d_loss : 0.410990\n",
      "epoch 2/2 batch 56/390 g_loss : 2.339845\n",
      "epoch 2/2 batch 56/390 d_loss : 0.416958\n",
      "epoch 2/2 batch 57/390 g_loss : 2.119950\n",
      "epoch 2/2 batch 57/390 d_loss : 0.357341\n",
      "epoch 2/2 batch 58/390 g_loss : 2.907856\n",
      "epoch 2/2 batch 58/390 d_loss : 0.396719\n",
      "epoch 2/2 batch 59/390 g_loss : 1.973924\n",
      "epoch 2/2 batch 59/390 d_loss : 0.372590\n",
      "epoch 2/2 batch 60/390 g_loss : 1.783023\n",
      "epoch 2/2 batch 60/390 d_loss : 0.362049\n",
      "epoch 2/2 batch 61/390 g_loss : 3.061454\n",
      "epoch 2/2 batch 61/390 d_loss : 0.347646\n",
      "epoch 2/2 batch 62/390 g_loss : 2.529544\n",
      "epoch 2/2 batch 62/390 d_loss : 0.376861\n",
      "epoch 2/2 batch 63/390 g_loss : 1.635222\n",
      "epoch 2/2 batch 63/390 d_loss : 0.431149\n",
      "epoch 2/2 batch 64/390 g_loss : 2.971626\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 64/390 d_loss : 0.345689\n",
      "epoch 2/2 batch 65/390 g_loss : 2.310597\n",
      "epoch 2/2 batch 65/390 d_loss : 0.353287\n",
      "epoch 2/2 batch 66/390 g_loss : 1.970588\n",
      "epoch 2/2 batch 66/390 d_loss : 0.377349\n",
      "epoch 2/2 batch 67/390 g_loss : 2.796127\n",
      "epoch 2/2 batch 67/390 d_loss : 0.366897\n",
      "epoch 2/2 batch 68/390 g_loss : 2.637704\n",
      "epoch 2/2 batch 68/390 d_loss : 0.373166\n",
      "epoch 2/2 batch 69/390 g_loss : 1.330661\n",
      "epoch 2/2 batch 69/390 d_loss : 0.467717\n",
      "epoch 2/2 batch 70/390 g_loss : 3.057266\n",
      "epoch 2/2 batch 70/390 d_loss : 0.372389\n",
      "epoch 2/2 batch 71/390 g_loss : 2.180302\n",
      "epoch 2/2 batch 71/390 d_loss : 0.432274\n",
      "epoch 2/2 batch 72/390 g_loss : 2.939656\n",
      "epoch 2/2 batch 72/390 d_loss : 0.376883\n",
      "epoch 2/2 batch 73/390 g_loss : 2.054580\n",
      "epoch 2/2 batch 73/390 d_loss : 0.388099\n",
      "epoch 2/2 batch 74/390 g_loss : 1.946032\n",
      "epoch 2/2 batch 74/390 d_loss : 0.366237\n",
      "epoch 2/2 batch 75/390 g_loss : 2.661475\n",
      "epoch 2/2 batch 75/390 d_loss : 0.387551\n",
      "epoch 2/2 batch 76/390 g_loss : 2.746347\n",
      "epoch 2/2 batch 76/390 d_loss : 0.428412\n",
      "epoch 2/2 batch 77/390 g_loss : 1.714676\n",
      "epoch 2/2 batch 77/390 d_loss : 0.513568\n",
      "epoch 2/2 batch 78/390 g_loss : 2.622960\n",
      "epoch 2/2 batch 78/390 d_loss : 0.380330\n",
      "epoch 2/2 batch 79/390 g_loss : 2.895492\n",
      "epoch 2/2 batch 79/390 d_loss : 0.312324\n",
      "epoch 2/2 batch 80/390 g_loss : 2.805011\n",
      "epoch 2/2 batch 80/390 d_loss : 0.359452\n",
      "epoch 2/2 batch 81/390 g_loss : 2.136407\n",
      "epoch 2/2 batch 81/390 d_loss : 0.389576\n",
      "epoch 2/2 batch 82/390 g_loss : 2.881070\n",
      "epoch 2/2 batch 82/390 d_loss : 0.452561\n",
      "epoch 2/2 batch 83/390 g_loss : 2.114734\n",
      "epoch 2/2 batch 83/390 d_loss : 0.375013\n",
      "epoch 2/2 batch 84/390 g_loss : 2.420243\n",
      "epoch 2/2 batch 84/390 d_loss : 0.369226\n",
      "epoch 2/2 batch 85/390 g_loss : 2.966701\n",
      "epoch 2/2 batch 85/390 d_loss : 0.371531\n",
      "epoch 2/2 batch 86/390 g_loss : 2.352175\n",
      "epoch 2/2 batch 86/390 d_loss : 0.352787\n",
      "epoch 2/2 batch 87/390 g_loss : 2.527031\n",
      "epoch 2/2 batch 87/390 d_loss : 0.392123\n",
      "epoch 2/2 batch 88/390 g_loss : 2.134298\n",
      "epoch 2/2 batch 88/390 d_loss : 0.401083\n",
      "epoch 2/2 batch 89/390 g_loss : 2.868016\n",
      "epoch 2/2 batch 89/390 d_loss : 0.381914\n",
      "epoch 2/2 batch 90/390 g_loss : 2.052264\n",
      "epoch 2/2 batch 90/390 d_loss : 0.383793\n",
      "epoch 2/2 batch 91/390 g_loss : 2.247546\n",
      "epoch 2/2 batch 91/390 d_loss : 0.415893\n",
      "epoch 2/2 batch 92/390 g_loss : 2.656995\n",
      "epoch 2/2 batch 92/390 d_loss : 0.350898\n",
      "epoch 2/2 batch 93/390 g_loss : 2.568173\n",
      "epoch 2/2 batch 93/390 d_loss : 0.364879\n",
      "epoch 2/2 batch 94/390 g_loss : 2.333919\n",
      "epoch 2/2 batch 94/390 d_loss : 0.348572\n",
      "epoch 2/2 batch 95/390 g_loss : 2.100890\n",
      "epoch 2/2 batch 95/390 d_loss : 0.343292\n",
      "epoch 2/2 batch 96/390 g_loss : 2.223721\n",
      "epoch 2/2 batch 96/390 d_loss : 0.347759\n",
      "epoch 2/2 batch 97/390 g_loss : 2.871769\n",
      "epoch 2/2 batch 97/390 d_loss : 0.339203\n",
      "epoch 2/2 batch 98/390 g_loss : 2.410666\n",
      "epoch 2/2 batch 98/390 d_loss : 0.356120\n",
      "epoch 2/2 batch 99/390 g_loss : 2.278080\n",
      "epoch 2/2 batch 99/390 d_loss : 0.484089\n",
      "epoch 2/2 batch 100/390 g_loss : 2.894382\n",
      "epoch 2/2 batch 100/390 d_loss : 0.361045\n",
      "epoch 2/2 batch 101/390 g_loss : 1.960718\n",
      "epoch 2/2 batch 101/390 d_loss : 0.504205\n",
      "epoch 2/2 batch 102/390 g_loss : 3.674863\n",
      "epoch 2/2 batch 102/390 d_loss : 0.579472\n",
      "epoch 2/2 batch 103/390 g_loss : 1.764237\n",
      "epoch 2/2 batch 103/390 d_loss : 0.456004\n",
      "epoch 2/2 batch 104/390 g_loss : 2.205395\n",
      "epoch 2/2 batch 104/390 d_loss : 0.413101\n",
      "epoch 2/2 batch 105/390 g_loss : 2.918858\n",
      "epoch 2/2 batch 105/390 d_loss : 0.397381\n",
      "epoch 2/2 batch 106/390 g_loss : 1.980470\n",
      "epoch 2/2 batch 106/390 d_loss : 0.450705\n",
      "epoch 2/2 batch 107/390 g_loss : 1.933243\n",
      "epoch 2/2 batch 107/390 d_loss : 0.404396\n",
      "epoch 2/2 batch 108/390 g_loss : 2.227625\n",
      "epoch 2/2 batch 108/390 d_loss : 0.348877\n",
      "epoch 2/2 batch 109/390 g_loss : 2.674486\n",
      "epoch 2/2 batch 109/390 d_loss : 0.368349\n",
      "epoch 2/2 batch 110/390 g_loss : 2.027917\n",
      "epoch 2/2 batch 110/390 d_loss : 0.393441\n",
      "epoch 2/2 batch 111/390 g_loss : 2.461428\n",
      "epoch 2/2 batch 111/390 d_loss : 0.319067\n",
      "epoch 2/2 batch 112/390 g_loss : 2.725540\n",
      "epoch 2/2 batch 112/390 d_loss : 0.341681\n",
      "epoch 2/2 batch 113/390 g_loss : 2.499720\n",
      "epoch 2/2 batch 113/390 d_loss : 0.348884\n",
      "epoch 2/2 batch 114/390 g_loss : 1.922314\n",
      "epoch 2/2 batch 114/390 d_loss : 0.400186\n",
      "epoch 2/2 batch 115/390 g_loss : 3.025638\n",
      "epoch 2/2 batch 115/390 d_loss : 0.404965\n",
      "epoch 2/2 batch 116/390 g_loss : 2.270060\n",
      "epoch 2/2 batch 116/390 d_loss : 0.331565\n",
      "epoch 2/2 batch 117/390 g_loss : 2.049299\n",
      "epoch 2/2 batch 117/390 d_loss : 0.407738\n",
      "epoch 2/2 batch 118/390 g_loss : 2.752391\n",
      "epoch 2/2 batch 118/390 d_loss : 0.323295\n",
      "epoch 2/2 batch 119/390 g_loss : 1.973288\n",
      "epoch 2/2 batch 119/390 d_loss : 0.367887\n",
      "epoch 2/2 batch 120/390 g_loss : 2.451242\n",
      "epoch 2/2 batch 120/390 d_loss : 0.336801\n",
      "epoch 2/2 batch 121/390 g_loss : 2.185900\n",
      "epoch 2/2 batch 121/390 d_loss : 0.322654\n",
      "epoch 2/2 batch 122/390 g_loss : 2.175525\n",
      "epoch 2/2 batch 122/390 d_loss : 0.395838\n",
      "epoch 2/2 batch 123/390 g_loss : 2.947205\n",
      "epoch 2/2 batch 123/390 d_loss : 0.420849\n",
      "epoch 2/2 batch 124/390 g_loss : 1.214704\n",
      "epoch 2/2 batch 124/390 d_loss : 0.570093\n",
      "epoch 2/2 batch 125/390 g_loss : 3.497525\n",
      "epoch 2/2 batch 125/390 d_loss : 0.420716\n",
      "epoch 2/2 batch 126/390 g_loss : 1.711334\n",
      "epoch 2/2 batch 126/390 d_loss : 0.469262\n",
      "epoch 2/2 batch 127/390 g_loss : 3.577036\n",
      "epoch 2/2 batch 127/390 d_loss : 0.436466\n",
      "epoch 2/2 batch 128/390 g_loss : 1.703016\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 128/390 d_loss : 0.494838\n",
      "epoch 2/2 batch 129/390 g_loss : 2.383708\n",
      "epoch 2/2 batch 129/390 d_loss : 0.333504\n",
      "epoch 2/2 batch 130/390 g_loss : 2.540356\n",
      "epoch 2/2 batch 130/390 d_loss : 0.377810\n",
      "epoch 2/2 batch 131/390 g_loss : 1.977963\n",
      "epoch 2/2 batch 131/390 d_loss : 0.410401\n",
      "epoch 2/2 batch 132/390 g_loss : 2.542887\n",
      "epoch 2/2 batch 132/390 d_loss : 0.346554\n",
      "epoch 2/2 batch 133/390 g_loss : 2.404512\n",
      "epoch 2/2 batch 133/390 d_loss : 0.404419\n",
      "epoch 2/2 batch 134/390 g_loss : 2.002360\n",
      "epoch 2/2 batch 134/390 d_loss : 0.377039\n",
      "epoch 2/2 batch 135/390 g_loss : 2.511245\n",
      "epoch 2/2 batch 135/390 d_loss : 0.341031\n",
      "epoch 2/2 batch 136/390 g_loss : 2.498251\n",
      "epoch 2/2 batch 136/390 d_loss : 0.403068\n",
      "epoch 2/2 batch 137/390 g_loss : 2.165127\n",
      "epoch 2/2 batch 137/390 d_loss : 0.363960\n",
      "epoch 2/2 batch 138/390 g_loss : 2.308772\n",
      "epoch 2/2 batch 138/390 d_loss : 0.384487\n",
      "epoch 2/2 batch 139/390 g_loss : 2.501389\n",
      "epoch 2/2 batch 139/390 d_loss : 0.410281\n",
      "epoch 2/2 batch 140/390 g_loss : 2.934801\n",
      "epoch 2/2 batch 140/390 d_loss : 0.392426\n",
      "epoch 2/2 batch 141/390 g_loss : 1.903033\n",
      "epoch 2/2 batch 141/390 d_loss : 0.395891\n",
      "epoch 2/2 batch 142/390 g_loss : 2.827749\n",
      "epoch 2/2 batch 142/390 d_loss : 0.311245\n",
      "epoch 2/2 batch 143/390 g_loss : 2.957365\n",
      "epoch 2/2 batch 143/390 d_loss : 0.386578\n",
      "epoch 2/2 batch 144/390 g_loss : 2.674377\n",
      "epoch 2/2 batch 144/390 d_loss : 0.360505\n",
      "epoch 2/2 batch 145/390 g_loss : 2.250680\n",
      "epoch 2/2 batch 145/390 d_loss : 0.350248\n",
      "epoch 2/2 batch 146/390 g_loss : 2.798178\n",
      "epoch 2/2 batch 146/390 d_loss : 0.424272\n",
      "epoch 2/2 batch 147/390 g_loss : 2.369411\n",
      "epoch 2/2 batch 147/390 d_loss : 0.366748\n",
      "epoch 2/2 batch 148/390 g_loss : 2.283574\n",
      "epoch 2/2 batch 148/390 d_loss : 0.354057\n",
      "epoch 2/2 batch 149/390 g_loss : 2.322224\n",
      "epoch 2/2 batch 149/390 d_loss : 0.364416\n",
      "epoch 2/2 batch 150/390 g_loss : 2.405883\n",
      "epoch 2/2 batch 150/390 d_loss : 0.325356\n",
      "epoch 2/2 batch 151/390 g_loss : 2.280351\n",
      "epoch 2/2 batch 151/390 d_loss : 0.361542\n",
      "epoch 2/2 batch 152/390 g_loss : 2.781094\n",
      "epoch 2/2 batch 152/390 d_loss : 0.419434\n",
      "epoch 2/2 batch 153/390 g_loss : 2.324022\n",
      "epoch 2/2 batch 153/390 d_loss : 0.371923\n",
      "epoch 2/2 batch 154/390 g_loss : 2.420774\n",
      "epoch 2/2 batch 154/390 d_loss : 0.326238\n",
      "epoch 2/2 batch 155/390 g_loss : 3.208310\n",
      "epoch 2/2 batch 155/390 d_loss : 0.436931\n",
      "epoch 2/2 batch 156/390 g_loss : 1.444834\n",
      "epoch 2/2 batch 156/390 d_loss : 0.437140\n",
      "epoch 2/2 batch 157/390 g_loss : 2.526684\n",
      "epoch 2/2 batch 157/390 d_loss : 0.377056\n",
      "epoch 2/2 batch 158/390 g_loss : 2.693221\n",
      "epoch 2/2 batch 158/390 d_loss : 0.400923\n",
      "epoch 2/2 batch 159/390 g_loss : 2.089199\n",
      "epoch 2/2 batch 159/390 d_loss : 0.378865\n",
      "epoch 2/2 batch 160/390 g_loss : 2.447159\n",
      "epoch 2/2 batch 160/390 d_loss : 0.373495\n",
      "epoch 2/2 batch 161/390 g_loss : 2.210541\n",
      "epoch 2/2 batch 161/390 d_loss : 0.375931\n",
      "epoch 2/2 batch 162/390 g_loss : 2.113011\n",
      "epoch 2/2 batch 162/390 d_loss : 0.363460\n",
      "epoch 2/2 batch 163/390 g_loss : 2.719865\n",
      "epoch 2/2 batch 163/390 d_loss : 0.346944\n",
      "epoch 2/2 batch 164/390 g_loss : 2.592014\n",
      "epoch 2/2 batch 164/390 d_loss : 0.371622\n",
      "epoch 2/2 batch 165/390 g_loss : 2.244818\n",
      "epoch 2/2 batch 165/390 d_loss : 0.368551\n",
      "epoch 2/2 batch 166/390 g_loss : 2.451507\n",
      "epoch 2/2 batch 166/390 d_loss : 0.326162\n",
      "epoch 2/2 batch 167/390 g_loss : 2.335335\n",
      "epoch 2/2 batch 167/390 d_loss : 0.332105\n",
      "epoch 2/2 batch 168/390 g_loss : 2.584361\n",
      "epoch 2/2 batch 168/390 d_loss : 0.337774\n",
      "epoch 2/2 batch 169/390 g_loss : 2.360294\n",
      "epoch 2/2 batch 169/390 d_loss : 0.341709\n",
      "epoch 2/2 batch 170/390 g_loss : 1.797832\n",
      "epoch 2/2 batch 170/390 d_loss : 0.340566\n",
      "epoch 2/2 batch 171/390 g_loss : 2.604104\n",
      "epoch 2/2 batch 171/390 d_loss : 0.351627\n",
      "epoch 2/2 batch 172/390 g_loss : 2.421282\n",
      "epoch 2/2 batch 172/390 d_loss : 0.321192\n",
      "epoch 2/2 batch 173/390 g_loss : 2.363071\n",
      "epoch 2/2 batch 173/390 d_loss : 0.343268\n",
      "epoch 2/2 batch 174/390 g_loss : 2.846826\n",
      "epoch 2/2 batch 174/390 d_loss : 0.344227\n",
      "epoch 2/2 batch 175/390 g_loss : 2.887654\n",
      "epoch 2/2 batch 175/390 d_loss : 0.359757\n",
      "epoch 2/2 batch 176/390 g_loss : 1.761642\n",
      "epoch 2/2 batch 176/390 d_loss : 0.404118\n",
      "epoch 2/2 batch 177/390 g_loss : 2.999519\n",
      "epoch 2/2 batch 177/390 d_loss : 0.379078\n",
      "epoch 2/2 batch 178/390 g_loss : 2.541968\n",
      "epoch 2/2 batch 178/390 d_loss : 0.408465\n",
      "epoch 2/2 batch 179/390 g_loss : 1.615670\n",
      "epoch 2/2 batch 179/390 d_loss : 0.382717\n",
      "epoch 2/2 batch 180/390 g_loss : 2.846703\n",
      "epoch 2/2 batch 180/390 d_loss : 0.374281\n",
      "epoch 2/2 batch 181/390 g_loss : 3.038893\n",
      "epoch 2/2 batch 181/390 d_loss : 0.369314\n",
      "epoch 2/2 batch 182/390 g_loss : 2.287540\n",
      "epoch 2/2 batch 182/390 d_loss : 0.389074\n",
      "epoch 2/2 batch 183/390 g_loss : 2.409226\n",
      "epoch 2/2 batch 183/390 d_loss : 0.363491\n",
      "epoch 2/2 batch 184/390 g_loss : 2.366904\n",
      "epoch 2/2 batch 184/390 d_loss : 0.357271\n",
      "epoch 2/2 batch 185/390 g_loss : 2.579381\n",
      "epoch 2/2 batch 185/390 d_loss : 0.381493\n",
      "epoch 2/2 batch 186/390 g_loss : 2.229574\n",
      "epoch 2/2 batch 186/390 d_loss : 0.366162\n",
      "epoch 2/2 batch 187/390 g_loss : 2.999655\n",
      "epoch 2/2 batch 187/390 d_loss : 0.433917\n",
      "epoch 2/2 batch 188/390 g_loss : 1.542823\n",
      "epoch 2/2 batch 188/390 d_loss : 0.461174\n",
      "epoch 2/2 batch 189/390 g_loss : 2.621829\n",
      "epoch 2/2 batch 189/390 d_loss : 0.356645\n",
      "epoch 2/2 batch 190/390 g_loss : 2.285008\n",
      "epoch 2/2 batch 190/390 d_loss : 0.331633\n",
      "epoch 2/2 batch 191/390 g_loss : 2.734706\n",
      "epoch 2/2 batch 191/390 d_loss : 0.364791\n",
      "epoch 2/2 batch 192/390 g_loss : 2.562301\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 192/390 d_loss : 0.366749\n",
      "epoch 2/2 batch 193/390 g_loss : 2.894423\n",
      "epoch 2/2 batch 193/390 d_loss : 0.365521\n",
      "epoch 2/2 batch 194/390 g_loss : 1.947861\n",
      "epoch 2/2 batch 194/390 d_loss : 0.369491\n",
      "epoch 2/2 batch 195/390 g_loss : 2.723991\n",
      "epoch 2/2 batch 195/390 d_loss : 0.405207\n",
      "epoch 2/2 batch 196/390 g_loss : 2.325884\n",
      "epoch 2/2 batch 196/390 d_loss : 0.317986\n",
      "epoch 2/2 batch 197/390 g_loss : 2.911741\n",
      "epoch 2/2 batch 197/390 d_loss : 0.360242\n",
      "epoch 2/2 batch 198/390 g_loss : 2.452797\n",
      "epoch 2/2 batch 198/390 d_loss : 0.348321\n",
      "epoch 2/2 batch 199/390 g_loss : 2.700761\n",
      "epoch 2/2 batch 199/390 d_loss : 0.408612\n",
      "epoch 2/2 batch 200/390 g_loss : 3.320634\n",
      "epoch 2/2 batch 200/390 d_loss : 0.361766\n",
      "epoch 2/2 batch 201/390 g_loss : 1.570082\n",
      "epoch 2/2 batch 201/390 d_loss : 0.468904\n",
      "epoch 2/2 batch 202/390 g_loss : 3.796840\n",
      "epoch 2/2 batch 202/390 d_loss : 0.500766\n",
      "epoch 2/2 batch 203/390 g_loss : 1.760390\n",
      "epoch 2/2 batch 203/390 d_loss : 0.388926\n",
      "epoch 2/2 batch 204/390 g_loss : 2.173044\n",
      "epoch 2/2 batch 204/390 d_loss : 0.399556\n",
      "epoch 2/2 batch 205/390 g_loss : 2.904027\n",
      "epoch 2/2 batch 205/390 d_loss : 0.431025\n",
      "epoch 2/2 batch 206/390 g_loss : 2.230036\n",
      "epoch 2/2 batch 206/390 d_loss : 0.382135\n",
      "epoch 2/2 batch 207/390 g_loss : 2.677877\n",
      "epoch 2/2 batch 207/390 d_loss : 0.329450\n",
      "epoch 2/2 batch 208/390 g_loss : 2.872462\n",
      "epoch 2/2 batch 208/390 d_loss : 0.369663\n",
      "epoch 2/2 batch 209/390 g_loss : 2.320781\n",
      "epoch 2/2 batch 209/390 d_loss : 0.422175\n",
      "epoch 2/2 batch 210/390 g_loss : 2.350207\n",
      "epoch 2/2 batch 210/390 d_loss : 0.371183\n",
      "epoch 2/2 batch 211/390 g_loss : 2.884469\n",
      "epoch 2/2 batch 211/390 d_loss : 0.393994\n",
      "epoch 2/2 batch 212/390 g_loss : 2.109148\n",
      "epoch 2/2 batch 212/390 d_loss : 0.355370\n",
      "epoch 2/2 batch 213/390 g_loss : 2.263983\n",
      "epoch 2/2 batch 213/390 d_loss : 0.381065\n",
      "epoch 2/2 batch 214/390 g_loss : 2.857863\n",
      "epoch 2/2 batch 214/390 d_loss : 0.370917\n",
      "epoch 2/2 batch 215/390 g_loss : 2.048267\n",
      "epoch 2/2 batch 215/390 d_loss : 0.370492\n",
      "epoch 2/2 batch 216/390 g_loss : 3.022273\n",
      "epoch 2/2 batch 216/390 d_loss : 0.387592\n",
      "epoch 2/2 batch 217/390 g_loss : 1.937892\n",
      "epoch 2/2 batch 217/390 d_loss : 0.382559\n",
      "epoch 2/2 batch 218/390 g_loss : 2.383523\n",
      "epoch 2/2 batch 218/390 d_loss : 0.376420\n",
      "epoch 2/2 batch 219/390 g_loss : 2.778370\n",
      "epoch 2/2 batch 219/390 d_loss : 0.379609\n",
      "epoch 2/2 batch 220/390 g_loss : 2.927411\n",
      "epoch 2/2 batch 220/390 d_loss : 0.383332\n",
      "epoch 2/2 batch 221/390 g_loss : 2.039315\n",
      "epoch 2/2 batch 221/390 d_loss : 0.420547\n",
      "epoch 2/2 batch 222/390 g_loss : 2.620671\n",
      "epoch 2/2 batch 222/390 d_loss : 0.403000\n",
      "epoch 2/2 batch 223/390 g_loss : 2.283328\n",
      "epoch 2/2 batch 223/390 d_loss : 0.413991\n",
      "epoch 2/2 batch 224/390 g_loss : 2.461788\n",
      "epoch 2/2 batch 224/390 d_loss : 0.351346\n",
      "epoch 2/2 batch 225/390 g_loss : 2.669317\n",
      "epoch 2/2 batch 225/390 d_loss : 0.313372\n",
      "epoch 2/2 batch 226/390 g_loss : 3.062246\n",
      "epoch 2/2 batch 226/390 d_loss : 0.394495\n",
      "epoch 2/2 batch 227/390 g_loss : 2.265824\n",
      "epoch 2/2 batch 227/390 d_loss : 0.355669\n",
      "epoch 2/2 batch 228/390 g_loss : 2.598908\n",
      "epoch 2/2 batch 228/390 d_loss : 0.309833\n",
      "epoch 2/2 batch 229/390 g_loss : 2.768253\n",
      "epoch 2/2 batch 229/390 d_loss : 0.325131\n",
      "epoch 2/2 batch 230/390 g_loss : 2.717099\n",
      "epoch 2/2 batch 230/390 d_loss : 0.353211\n",
      "epoch 2/2 batch 231/390 g_loss : 2.976411\n",
      "epoch 2/2 batch 231/390 d_loss : 0.392136\n",
      "epoch 2/2 batch 232/390 g_loss : 1.961467\n",
      "epoch 2/2 batch 232/390 d_loss : 0.498876\n",
      "epoch 2/2 batch 233/390 g_loss : 3.516098\n",
      "epoch 2/2 batch 233/390 d_loss : 0.469491\n",
      "epoch 2/2 batch 234/390 g_loss : 1.721866\n",
      "epoch 2/2 batch 234/390 d_loss : 0.508301\n",
      "epoch 2/2 batch 235/390 g_loss : 2.968372\n",
      "epoch 2/2 batch 235/390 d_loss : 0.472671\n",
      "epoch 2/2 batch 236/390 g_loss : 1.974238\n",
      "epoch 2/2 batch 236/390 d_loss : 0.449468\n",
      "epoch 2/2 batch 237/390 g_loss : 2.629925\n",
      "epoch 2/2 batch 237/390 d_loss : 0.388921\n",
      "epoch 2/2 batch 238/390 g_loss : 2.526655\n",
      "epoch 2/2 batch 238/390 d_loss : 0.353680\n",
      "epoch 2/2 batch 239/390 g_loss : 2.364865\n",
      "epoch 2/2 batch 239/390 d_loss : 0.420429\n",
      "epoch 2/2 batch 240/390 g_loss : 3.250365\n",
      "epoch 2/2 batch 240/390 d_loss : 0.450816\n",
      "epoch 2/2 batch 241/390 g_loss : 2.071856\n",
      "epoch 2/2 batch 241/390 d_loss : 0.405545\n",
      "epoch 2/2 batch 242/390 g_loss : 2.425059\n",
      "epoch 2/2 batch 242/390 d_loss : 0.370207\n",
      "epoch 2/2 batch 243/390 g_loss : 2.781772\n",
      "epoch 2/2 batch 243/390 d_loss : 0.387715\n",
      "epoch 2/2 batch 244/390 g_loss : 2.160332\n",
      "epoch 2/2 batch 244/390 d_loss : 0.338610\n",
      "epoch 2/2 batch 245/390 g_loss : 2.508976\n",
      "epoch 2/2 batch 245/390 d_loss : 0.349543\n",
      "epoch 2/2 batch 246/390 g_loss : 2.667156\n",
      "epoch 2/2 batch 246/390 d_loss : 0.357793\n",
      "epoch 2/2 batch 247/390 g_loss : 2.358649\n",
      "epoch 2/2 batch 247/390 d_loss : 0.380690\n",
      "epoch 2/2 batch 248/390 g_loss : 2.936604\n",
      "epoch 2/2 batch 248/390 d_loss : 0.368162\n",
      "epoch 2/2 batch 249/390 g_loss : 1.919163\n",
      "epoch 2/2 batch 249/390 d_loss : 0.394182\n",
      "epoch 2/2 batch 250/390 g_loss : 2.782659\n",
      "epoch 2/2 batch 250/390 d_loss : 0.356219\n",
      "epoch 2/2 batch 251/390 g_loss : 3.135105\n",
      "epoch 2/2 batch 251/390 d_loss : 0.365695\n",
      "epoch 2/2 batch 252/390 g_loss : 2.303598\n",
      "epoch 2/2 batch 252/390 d_loss : 0.320704\n",
      "epoch 2/2 batch 253/390 g_loss : 2.906215\n",
      "epoch 2/2 batch 253/390 d_loss : 0.370388\n",
      "epoch 2/2 batch 254/390 g_loss : 2.795142\n",
      "epoch 2/2 batch 254/390 d_loss : 0.385880\n",
      "epoch 2/2 batch 255/390 g_loss : 1.970238\n",
      "epoch 2/2 batch 255/390 d_loss : 0.433514\n",
      "epoch 2/2 batch 256/390 g_loss : 3.038955\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 256/390 d_loss : 0.403565\n",
      "epoch 2/2 batch 257/390 g_loss : 2.025163\n",
      "epoch 2/2 batch 257/390 d_loss : 0.366790\n",
      "epoch 2/2 batch 258/390 g_loss : 2.693451\n",
      "epoch 2/2 batch 258/390 d_loss : 0.404111\n",
      "epoch 2/2 batch 259/390 g_loss : 2.857376\n",
      "epoch 2/2 batch 259/390 d_loss : 0.406174\n",
      "epoch 2/2 batch 260/390 g_loss : 1.990525\n",
      "epoch 2/2 batch 260/390 d_loss : 0.348208\n",
      "epoch 2/2 batch 261/390 g_loss : 2.699231\n",
      "epoch 2/2 batch 261/390 d_loss : 0.333096\n",
      "epoch 2/2 batch 262/390 g_loss : 2.661021\n",
      "epoch 2/2 batch 262/390 d_loss : 0.352031\n",
      "epoch 2/2 batch 263/390 g_loss : 2.566686\n",
      "epoch 2/2 batch 263/390 d_loss : 0.356330\n",
      "epoch 2/2 batch 264/390 g_loss : 3.598434\n",
      "epoch 2/2 batch 264/390 d_loss : 0.440979\n",
      "epoch 2/2 batch 265/390 g_loss : 2.393190\n",
      "epoch 2/2 batch 265/390 d_loss : 0.324596\n",
      "epoch 2/2 batch 266/390 g_loss : 2.423512\n",
      "epoch 2/2 batch 266/390 d_loss : 0.417816\n",
      "epoch 2/2 batch 267/390 g_loss : 3.124472\n",
      "epoch 2/2 batch 267/390 d_loss : 0.419931\n",
      "epoch 2/2 batch 268/390 g_loss : 1.932131\n",
      "epoch 2/2 batch 268/390 d_loss : 0.447002\n",
      "epoch 2/2 batch 269/390 g_loss : 3.158252\n",
      "epoch 2/2 batch 269/390 d_loss : 0.428768\n",
      "epoch 2/2 batch 270/390 g_loss : 1.950312\n",
      "epoch 2/2 batch 270/390 d_loss : 0.410576\n",
      "epoch 2/2 batch 271/390 g_loss : 2.678390\n",
      "epoch 2/2 batch 271/390 d_loss : 0.400920\n",
      "epoch 2/2 batch 272/390 g_loss : 2.447447\n",
      "epoch 2/2 batch 272/390 d_loss : 0.446564\n",
      "epoch 2/2 batch 273/390 g_loss : 1.712903\n",
      "epoch 2/2 batch 273/390 d_loss : 0.438655\n",
      "epoch 2/2 batch 274/390 g_loss : 3.274561\n",
      "epoch 2/2 batch 274/390 d_loss : 0.312138\n",
      "epoch 2/2 batch 275/390 g_loss : 2.775700\n",
      "epoch 2/2 batch 275/390 d_loss : 0.371768\n",
      "epoch 2/2 batch 276/390 g_loss : 2.715615\n",
      "epoch 2/2 batch 276/390 d_loss : 0.382241\n",
      "epoch 2/2 batch 277/390 g_loss : 2.809977\n",
      "epoch 2/2 batch 277/390 d_loss : 0.387918\n",
      "epoch 2/2 batch 278/390 g_loss : 2.413378\n",
      "epoch 2/2 batch 278/390 d_loss : 0.405620\n",
      "epoch 2/2 batch 279/390 g_loss : 1.940071\n",
      "epoch 2/2 batch 279/390 d_loss : 0.403539\n",
      "epoch 2/2 batch 280/390 g_loss : 2.776710\n",
      "epoch 2/2 batch 280/390 d_loss : 0.384118\n",
      "epoch 2/2 batch 281/390 g_loss : 2.419229\n",
      "epoch 2/2 batch 281/390 d_loss : 0.397905\n",
      "epoch 2/2 batch 282/390 g_loss : 2.075047\n",
      "epoch 2/2 batch 282/390 d_loss : 0.402870\n",
      "epoch 2/2 batch 283/390 g_loss : 2.839032\n",
      "epoch 2/2 batch 283/390 d_loss : 0.370497\n",
      "epoch 2/2 batch 284/390 g_loss : 2.581687\n",
      "epoch 2/2 batch 284/390 d_loss : 0.347083\n",
      "epoch 2/2 batch 285/390 g_loss : 2.375224\n",
      "epoch 2/2 batch 285/390 d_loss : 0.385097\n",
      "epoch 2/2 batch 286/390 g_loss : 2.648358\n",
      "epoch 2/2 batch 286/390 d_loss : 0.370972\n",
      "epoch 2/2 batch 287/390 g_loss : 2.572038\n",
      "epoch 2/2 batch 287/390 d_loss : 0.327571\n",
      "epoch 2/2 batch 288/390 g_loss : 3.176771\n",
      "epoch 2/2 batch 288/390 d_loss : 0.363874\n",
      "epoch 2/2 batch 289/390 g_loss : 2.210932\n",
      "epoch 2/2 batch 289/390 d_loss : 0.416101\n",
      "epoch 2/2 batch 290/390 g_loss : 2.580363\n",
      "epoch 2/2 batch 290/390 d_loss : 0.350963\n",
      "epoch 2/2 batch 291/390 g_loss : 3.015668\n",
      "epoch 2/2 batch 291/390 d_loss : 0.326333\n",
      "epoch 2/2 batch 292/390 g_loss : 2.996876\n",
      "epoch 2/2 batch 292/390 d_loss : 0.329427\n",
      "epoch 2/2 batch 293/390 g_loss : 3.559853\n",
      "epoch 2/2 batch 293/390 d_loss : 0.301414\n",
      "epoch 2/2 batch 294/390 g_loss : 2.670849\n",
      "epoch 2/2 batch 294/390 d_loss : 0.348969\n",
      "epoch 2/2 batch 295/390 g_loss : 2.329358\n",
      "epoch 2/2 batch 295/390 d_loss : 0.459919\n",
      "epoch 2/2 batch 296/390 g_loss : 4.036899\n",
      "epoch 2/2 batch 296/390 d_loss : 0.579029\n",
      "epoch 2/2 batch 297/390 g_loss : 1.503950\n",
      "epoch 2/2 batch 297/390 d_loss : 0.477627\n",
      "epoch 2/2 batch 298/390 g_loss : 2.716270\n",
      "epoch 2/2 batch 298/390 d_loss : 0.381973\n",
      "epoch 2/2 batch 299/390 g_loss : 3.147075\n",
      "epoch 2/2 batch 299/390 d_loss : 0.408076\n",
      "epoch 2/2 batch 300/390 g_loss : 2.127671\n",
      "epoch 2/2 batch 300/390 d_loss : 0.421354\n",
      "epoch 2/2 batch 301/390 g_loss : 2.438513\n",
      "epoch 2/2 batch 301/390 d_loss : 0.377676\n",
      "epoch 2/2 batch 302/390 g_loss : 2.492427\n",
      "epoch 2/2 batch 302/390 d_loss : 0.362005\n",
      "epoch 2/2 batch 303/390 g_loss : 2.303825\n",
      "epoch 2/2 batch 303/390 d_loss : 0.403727\n",
      "epoch 2/2 batch 304/390 g_loss : 2.722671\n",
      "epoch 2/2 batch 304/390 d_loss : 0.394044\n",
      "epoch 2/2 batch 305/390 g_loss : 3.008421\n",
      "epoch 2/2 batch 305/390 d_loss : 0.353387\n",
      "epoch 2/2 batch 306/390 g_loss : 2.668288\n",
      "epoch 2/2 batch 306/390 d_loss : 0.372144\n",
      "epoch 2/2 batch 307/390 g_loss : 2.161916\n",
      "epoch 2/2 batch 307/390 d_loss : 0.352285\n",
      "epoch 2/2 batch 308/390 g_loss : 2.881630\n",
      "epoch 2/2 batch 308/390 d_loss : 0.283328\n",
      "epoch 2/2 batch 309/390 g_loss : 2.599141\n",
      "epoch 2/2 batch 309/390 d_loss : 0.322200\n",
      "epoch 2/2 batch 310/390 g_loss : 2.773202\n",
      "epoch 2/2 batch 310/390 d_loss : 0.266845\n",
      "epoch 2/2 batch 311/390 g_loss : 2.981042\n",
      "epoch 2/2 batch 311/390 d_loss : 0.355560\n",
      "epoch 2/2 batch 312/390 g_loss : 2.628544\n",
      "epoch 2/2 batch 312/390 d_loss : 0.371450\n",
      "epoch 2/2 batch 313/390 g_loss : 2.507256\n",
      "epoch 2/2 batch 313/390 d_loss : 0.344654\n",
      "epoch 2/2 batch 314/390 g_loss : 2.526779\n",
      "epoch 2/2 batch 314/390 d_loss : 0.329649\n",
      "epoch 2/2 batch 315/390 g_loss : 3.140083\n",
      "epoch 2/2 batch 315/390 d_loss : 0.363456\n",
      "epoch 2/2 batch 316/390 g_loss : 2.864190\n",
      "epoch 2/2 batch 316/390 d_loss : 0.316722\n",
      "epoch 2/2 batch 317/390 g_loss : 2.331213\n",
      "epoch 2/2 batch 317/390 d_loss : 0.348036\n",
      "epoch 2/2 batch 318/390 g_loss : 3.030887\n",
      "epoch 2/2 batch 318/390 d_loss : 0.363522\n",
      "epoch 2/2 batch 319/390 g_loss : 3.462549\n",
      "epoch 2/2 batch 319/390 d_loss : 0.388773\n",
      "epoch 2/2 batch 320/390 g_loss : 2.343616\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 320/390 d_loss : 0.520644\n",
      "epoch 2/2 batch 321/390 g_loss : 2.801447\n",
      "epoch 2/2 batch 321/390 d_loss : 0.444366\n",
      "epoch 2/2 batch 322/390 g_loss : 2.287598\n",
      "epoch 2/2 batch 322/390 d_loss : 0.437721\n",
      "epoch 2/2 batch 323/390 g_loss : 3.377344\n",
      "epoch 2/2 batch 323/390 d_loss : 0.494720\n",
      "epoch 2/2 batch 324/390 g_loss : 2.210554\n",
      "epoch 2/2 batch 324/390 d_loss : 0.386116\n",
      "epoch 2/2 batch 325/390 g_loss : 2.196367\n",
      "epoch 2/2 batch 325/390 d_loss : 0.367289\n",
      "epoch 2/2 batch 326/390 g_loss : 2.614195\n",
      "epoch 2/2 batch 326/390 d_loss : 0.380179\n",
      "epoch 2/2 batch 327/390 g_loss : 2.345695\n",
      "epoch 2/2 batch 327/390 d_loss : 0.342915\n",
      "epoch 2/2 batch 328/390 g_loss : 2.494867\n",
      "epoch 2/2 batch 328/390 d_loss : 0.412865\n",
      "epoch 2/2 batch 329/390 g_loss : 2.309308\n",
      "epoch 2/2 batch 329/390 d_loss : 0.349453\n",
      "epoch 2/2 batch 330/390 g_loss : 2.281918\n",
      "epoch 2/2 batch 330/390 d_loss : 0.347283\n",
      "epoch 2/2 batch 331/390 g_loss : 2.209610\n",
      "epoch 2/2 batch 331/390 d_loss : 0.314238\n",
      "epoch 2/2 batch 332/390 g_loss : 2.477863\n",
      "epoch 2/2 batch 332/390 d_loss : 0.392388\n",
      "epoch 2/2 batch 333/390 g_loss : 2.389979\n",
      "epoch 2/2 batch 333/390 d_loss : 0.405083\n",
      "epoch 2/2 batch 334/390 g_loss : 2.361103\n",
      "epoch 2/2 batch 334/390 d_loss : 0.361404\n",
      "epoch 2/2 batch 335/390 g_loss : 2.808758\n",
      "epoch 2/2 batch 335/390 d_loss : 0.336485\n",
      "epoch 2/2 batch 336/390 g_loss : 2.097379\n",
      "epoch 2/2 batch 336/390 d_loss : 0.368793\n",
      "epoch 2/2 batch 337/390 g_loss : 2.653380\n",
      "epoch 2/2 batch 337/390 d_loss : 0.333608\n",
      "epoch 2/2 batch 338/390 g_loss : 2.536013\n",
      "epoch 2/2 batch 338/390 d_loss : 0.387560\n",
      "epoch 2/2 batch 339/390 g_loss : 2.261327\n",
      "epoch 2/2 batch 339/390 d_loss : 0.397719\n",
      "epoch 2/2 batch 340/390 g_loss : 2.638129\n",
      "epoch 2/2 batch 340/390 d_loss : 0.302315\n",
      "epoch 2/2 batch 341/390 g_loss : 2.356997\n",
      "epoch 2/2 batch 341/390 d_loss : 0.361941\n",
      "epoch 2/2 batch 342/390 g_loss : 2.705580\n",
      "epoch 2/2 batch 342/390 d_loss : 0.389859\n",
      "epoch 2/2 batch 343/390 g_loss : 2.001791\n",
      "epoch 2/2 batch 343/390 d_loss : 0.390013\n",
      "epoch 2/2 batch 344/390 g_loss : 2.685231\n",
      "epoch 2/2 batch 344/390 d_loss : 0.353593\n",
      "epoch 2/2 batch 345/390 g_loss : 2.138609\n",
      "epoch 2/2 batch 345/390 d_loss : 0.365561\n",
      "epoch 2/2 batch 346/390 g_loss : 2.357550\n",
      "epoch 2/2 batch 346/390 d_loss : 0.318333\n",
      "epoch 2/2 batch 347/390 g_loss : 2.508111\n",
      "epoch 2/2 batch 347/390 d_loss : 0.336005\n",
      "epoch 2/2 batch 348/390 g_loss : 2.301276\n",
      "epoch 2/2 batch 348/390 d_loss : 0.400876\n",
      "epoch 2/2 batch 349/390 g_loss : 2.562511\n",
      "epoch 2/2 batch 349/390 d_loss : 0.346737\n",
      "epoch 2/2 batch 350/390 g_loss : 2.222872\n",
      "epoch 2/2 batch 350/390 d_loss : 0.353296\n",
      "epoch 2/2 batch 351/390 g_loss : 2.954257\n",
      "epoch 2/2 batch 351/390 d_loss : 0.337875\n",
      "epoch 2/2 batch 352/390 g_loss : 2.211538\n",
      "epoch 2/2 batch 352/390 d_loss : 0.310370\n",
      "epoch 2/2 batch 353/390 g_loss : 2.806307\n",
      "epoch 2/2 batch 353/390 d_loss : 0.373599\n",
      "epoch 2/2 batch 354/390 g_loss : 3.659246\n",
      "epoch 2/2 batch 354/390 d_loss : 0.402797\n",
      "epoch 2/2 batch 355/390 g_loss : 2.266375\n",
      "epoch 2/2 batch 355/390 d_loss : 0.349359\n",
      "epoch 2/2 batch 356/390 g_loss : 2.521498\n",
      "epoch 2/2 batch 356/390 d_loss : 0.416230\n",
      "epoch 2/2 batch 357/390 g_loss : 2.766939\n",
      "epoch 2/2 batch 357/390 d_loss : 0.357488\n",
      "epoch 2/2 batch 358/390 g_loss : 2.479836\n",
      "epoch 2/2 batch 358/390 d_loss : 0.360897\n",
      "epoch 2/2 batch 359/390 g_loss : 2.563478\n",
      "epoch 2/2 batch 359/390 d_loss : 0.391884\n",
      "epoch 2/2 batch 360/390 g_loss : 2.497025\n",
      "epoch 2/2 batch 360/390 d_loss : 0.346895\n",
      "epoch 2/2 batch 361/390 g_loss : 2.769462\n",
      "epoch 2/2 batch 361/390 d_loss : 0.347392\n",
      "epoch 2/2 batch 362/390 g_loss : 2.597139\n",
      "epoch 2/2 batch 362/390 d_loss : 0.361276\n",
      "epoch 2/2 batch 363/390 g_loss : 2.115497\n",
      "epoch 2/2 batch 363/390 d_loss : 0.416807\n",
      "epoch 2/2 batch 364/390 g_loss : 3.193915\n",
      "epoch 2/2 batch 364/390 d_loss : 0.460350\n",
      "epoch 2/2 batch 365/390 g_loss : 1.620258\n",
      "epoch 2/2 batch 365/390 d_loss : 0.480737\n",
      "epoch 2/2 batch 366/390 g_loss : 2.887702\n",
      "epoch 2/2 batch 366/390 d_loss : 0.397326\n",
      "epoch 2/2 batch 367/390 g_loss : 3.046768\n",
      "epoch 2/2 batch 367/390 d_loss : 0.367975\n",
      "epoch 2/2 batch 368/390 g_loss : 1.768098\n",
      "epoch 2/2 batch 368/390 d_loss : 0.436757\n",
      "epoch 2/2 batch 369/390 g_loss : 2.860659\n",
      "epoch 2/2 batch 369/390 d_loss : 0.420154\n",
      "epoch 2/2 batch 370/390 g_loss : 2.039813\n",
      "epoch 2/2 batch 370/390 d_loss : 0.391853\n",
      "epoch 2/2 batch 371/390 g_loss : 2.986952\n",
      "epoch 2/2 batch 371/390 d_loss : 0.431175\n",
      "epoch 2/2 batch 372/390 g_loss : 1.645009\n",
      "epoch 2/2 batch 372/390 d_loss : 0.459078\n",
      "epoch 2/2 batch 373/390 g_loss : 2.951685\n",
      "epoch 2/2 batch 373/390 d_loss : 0.401921\n",
      "epoch 2/2 batch 374/390 g_loss : 2.416565\n",
      "epoch 2/2 batch 374/390 d_loss : 0.363996\n",
      "epoch 2/2 batch 375/390 g_loss : 2.197705\n",
      "epoch 2/2 batch 375/390 d_loss : 0.430005\n",
      "epoch 2/2 batch 376/390 g_loss : 3.036452\n",
      "epoch 2/2 batch 376/390 d_loss : 0.432285\n",
      "epoch 2/2 batch 377/390 g_loss : 2.114431\n",
      "epoch 2/2 batch 377/390 d_loss : 0.372138\n",
      "epoch 2/2 batch 378/390 g_loss : 2.004638\n",
      "epoch 2/2 batch 378/390 d_loss : 0.354858\n",
      "epoch 2/2 batch 379/390 g_loss : 2.967453\n",
      "epoch 2/2 batch 379/390 d_loss : 0.367516\n",
      "epoch 2/2 batch 380/390 g_loss : 2.781270\n",
      "epoch 2/2 batch 380/390 d_loss : 0.326347\n",
      "epoch 2/2 batch 381/390 g_loss : 2.522157\n",
      "epoch 2/2 batch 381/390 d_loss : 0.370481\n",
      "epoch 2/2 batch 382/390 g_loss : 2.421486\n",
      "epoch 2/2 batch 382/390 d_loss : 0.383770\n",
      "epoch 2/2 batch 383/390 g_loss : 3.189447\n",
      "epoch 2/2 batch 383/390 d_loss : 0.410032\n",
      "epoch 2/2 batch 384/390 g_loss : 1.985900\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n",
      "epoch 2/2 batch 384/390 d_loss : 0.366287\n",
      "epoch 2/2 batch 385/390 g_loss : 2.421911\n",
      "epoch 2/2 batch 385/390 d_loss : 0.405438\n",
      "epoch 2/2 batch 386/390 g_loss : 2.207335\n",
      "epoch 2/2 batch 386/390 d_loss : 0.377678\n",
      "epoch 2/2 batch 387/390 g_loss : 3.086612\n",
      "epoch 2/2 batch 387/390 d_loss : 0.376103\n",
      "epoch 2/2 batch 388/390 g_loss : 2.383513\n",
      "epoch 2/2 batch 388/390 d_loss : 0.370447\n",
      "epoch 2/2 batch 389/390 g_loss : 2.651092\n",
      "epoch 2/2 batch 389/390 d_loss : 0.374652\n"
     ]
    }
   ],
   "source": [
    "#Training the model - python cifar_gan.py --mode train --batch_size 128 --epoch_num 200\n",
    "train(BATCH_SIZE=128, epoch_num=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_1 (Dense)                  (None, 2048)          206848      dense_input_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 2048)          8192        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 2048)          0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 512, 2, 2)     0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_1 (UpSampling2D)    (None, 512, 4, 4)     0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 256, 4, 4)     3277056     upsampling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 256, 4, 4)     16          convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 256, 4, 4)     0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_2 (UpSampling2D)    (None, 256, 8, 8)     0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 128, 8, 8)     819328      upsampling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 128, 8, 8)     32          convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 128, 8, 8)     0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_3 (UpSampling2D)    (None, 128, 16, 16)   0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 64, 16, 16)    204864      upsampling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 64, 16, 16)    64          convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 64, 16, 16)    0           batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "upsampling2d_4 (UpSampling2D)    (None, 64, 32, 32)    0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 3, 32, 32)     4803        upsampling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 3, 32, 32)     0           convolution2d_4[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 4,521,203\n",
      "Trainable params: 4,517,051\n",
      "Non-trainable params: 4,152\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "128/128 [==============================] - 2s     \n",
      "after generator predict generated images shape (128, 3, 32, 32)\n",
      "num  128\n",
      "width  11\n",
      "height  12\n",
      "generated images shape before reshape  (128, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "generate(BATCH_SIZE=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Completed Traiing and Generation\n",
    "### Begin Semi Supervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "#import numpy as np\n",
    "#import keras\n",
    "#from keras.utils import to_categorical\n",
    "#from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "##from keras.layers.core import Activation\n",
    "#from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Convolution2D, MaxPooling2D\n",
    "#from keras.layers.advanced_activations import LeakyReLU\n",
    "import keras.regularizers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "#epochs = 5\n",
    "\n",
    "#mnist image dimensionality\n",
    "img_rows = 32\n",
    "img_cols = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 102, 1: 112, 2: 99, 3: 92, 4: 99, 5: 85, 6: 107, 7: 102, 8: 99, 9: 103}\n",
      "{0: 20, 1: 14, 2: 21, 3: 19, 4: 15, 5: 18, 6: 26, 7: 18, 8: 28, 9: 21}\n",
      "x_train shape: (1000, 3, 32, 32)\n",
      "x_test shape: (200, 3, 32, 32)\n",
      "1000 train samples\n",
      "200 test samples\n",
      "Y_train shape: (1000, 10)\n",
      "Y_test shape: (200, 10)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#loading the mnist dataInit\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "divided_input = np.array_split(X_train, 50)\n",
    "X_train = divided_input[0]\n",
    "\n",
    "\n",
    "divided_output = np.array_split(Y_train, 50)\n",
    "Y_train = divided_output[0]\n",
    "\n",
    "unique, counts = numpy.unique(Y_train, return_counts=True)\n",
    "print (dict(zip(unique, counts)))\n",
    "\n",
    "\n",
    "divided_inputtest = np.array_split(X_test, 50)\n",
    "X_test = divided_inputtest[0]\n",
    "divided_outputtest = np.array_split(Y_test, 50)\n",
    "Y_test = divided_outputtest[0]\n",
    "\n",
    "unique, counts = numpy.unique(Y_test, return_counts=True)\n",
    "print (dict(zip(unique, counts)))\n",
    "\n",
    "#reshaping for input to network\n",
    "#X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)\n",
    "#X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)\n",
    "\n",
    "#input_shape = (img_rows, img_cols, 3)\n",
    "input_shape = (3, img_rows, img_cols)\n",
    "\n",
    "#making data float datatype\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "#normalizing the data\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('x_train shape:', X_train.shape)\n",
    "print('x_test shape:', X_test.shape)\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "#convert class vectors to one hot encoded vectors\n",
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes)\n",
    "\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compilation successful\n"
     ]
    }
   ],
   "source": [
    "feature_layers = [\n",
    "    Convolution2D(64, 5, 5, border_mode='same',subsample=(2, 2), input_shape=(3,32,32)),\n",
    "    LeakyReLU(0.2),\n",
    "    Dropout(0.5),\n",
    "    Convolution2D(128, 5, 5, border_mode='same', subsample=(2,2)),\n",
    "    LeakyReLU(0.2),\n",
    "    Dropout(0.5),\n",
    "    Convolution2D(256, 5, 5, border_mode='same', subsample=(2,2)),\n",
    "    LeakyReLU(0.2),\n",
    "    Dropout(0.5),\n",
    "    Convolution2D(512, 5, 5, border_mode='same', subsample=(4,4)),\n",
    "    LeakyReLU(0.2),\n",
    "    Dropout(0.5),\n",
    "    Flatten()\n",
    "]\n",
    "'''\n",
    "classification_layers = [\n",
    "    Dense(512, W_regularizer=keras.regularizers.l2(0.01), name='fc_layer1'),\n",
    "    Activation('relu'),\n",
    "    Dense(num_classes, activation='softmax', W_regularizer=keras.regularizers.l2(0.01), name='fc_layer2')\n",
    "]\n",
    "'''\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(512, W_regularizer=keras.regularizers.l2(0.01), name='fc_layer1'),\n",
    "    Activation('relu'),\n",
    "    Dense(num_classes, activation='softmax', name='fc_layer2')\n",
    "]\n",
    "\n",
    "model = Sequential(feature_layers + classification_layers)\n",
    "# different backend has different image dim order, so we need to judge first.\n",
    "'''\n",
    "input_shape = (28,28,1)\n",
    "model.add(Convolution2D(64, 5, 5, border_mode='same',subsample=(2, 2), input_shape=input_shape))\n",
    "#model.add(LeakyReLU(0.02))\n",
    "model.add(Activation('tanh'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 5, 5, border_mode='same', subsample=(2,2)))\n",
    "#model.add(LeakyReLU(0.02))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "#model.add(LeakyReLU(0.02))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#model.add(Dense(1))\n",
    "#model.add(Activation('sigmoid'))\n",
    "'''\n",
    "#print model.summary()\n",
    "\n",
    "model.load_weights('discriminator_cifar', by_name=True)\n",
    "\n",
    "for l in feature_layers:\n",
    "    l.trainable = False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Model Compilation successful')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_13 (Convolution2D) (None, 64, 16, 16)    4864        convolution2d_input_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_9 (LeakyReLU)          (None, 64, 16, 16)    0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 64, 16, 16)    0           leakyrelu_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_14 (Convolution2D) (None, 128, 8, 8)     204928      dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_10 (LeakyReLU)         (None, 128, 8, 8)     0           convolution2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 128, 8, 8)     0           leakyrelu_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_15 (Convolution2D) (None, 256, 4, 4)     819456      dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_11 (LeakyReLU)         (None, 256, 4, 4)     0           convolution2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 256, 4, 4)     0           leakyrelu_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_16 (Convolution2D) (None, 512, 1, 1)     3277312     dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "leakyrelu_12 (LeakyReLU)         (None, 512, 1, 1)     0           convolution2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 512, 1, 1)     0           leakyrelu_12[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 512)           0           dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "fc_layer1 (Dense)                (None, 512)           262656      flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 512)           0           fc_layer1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fc_layer2 (Dense)                (None, 10)            5130        activation_8[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 4,574,346\n",
      "Trainable params: 267,786\n",
      "Non-trainable params: 4,306,560\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s - loss: 6.8037 - acc: 0.1370 - val_loss: 6.0249 - val_acc: 0.2000\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s - loss: 5.5409 - acc: 0.1850 - val_loss: 4.9255 - val_acc: 0.2300\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s - loss: 4.5545 - acc: 0.2180 - val_loss: 4.0847 - val_acc: 0.3000\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s - loss: 3.8120 - acc: 0.2090 - val_loss: 3.4583 - val_acc: 0.2800\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s - loss: 3.2692 - acc: 0.2300 - val_loss: 3.0192 - val_acc: 0.2400\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.8790 - acc: 0.2360 - val_loss: 2.7048 - val_acc: 0.3100\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.6276 - acc: 0.2470 - val_loss: 2.4822 - val_acc: 0.3150\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.4246 - acc: 0.2640 - val_loss: 2.3425 - val_acc: 0.3150\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.3071 - acc: 0.2480 - val_loss: 2.2574 - val_acc: 0.2800\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.2282 - acc: 0.2470 - val_loss: 2.1864 - val_acc: 0.3050\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.1664 - acc: 0.2820 - val_loss: 2.1411 - val_acc: 0.3150\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.1261 - acc: 0.2710 - val_loss: 2.1073 - val_acc: 0.3200\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.1145 - acc: 0.2620 - val_loss: 2.0856 - val_acc: 0.2950\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.1016 - acc: 0.2720 - val_loss: 2.0867 - val_acc: 0.2650\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0769 - acc: 0.2710 - val_loss: 2.0578 - val_acc: 0.3350\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0566 - acc: 0.2730 - val_loss: 2.0694 - val_acc: 0.3200\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0646 - acc: 0.2790 - val_loss: 2.0355 - val_acc: 0.3200\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0742 - acc: 0.2260 - val_loss: 2.0434 - val_acc: 0.3500\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0377 - acc: 0.2720 - val_loss: 2.0361 - val_acc: 0.3450\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0575 - acc: 0.2840 - val_loss: 2.0261 - val_acc: 0.3050\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0338 - acc: 0.2790 - val_loss: 2.0314 - val_acc: 0.3250\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0326 - acc: 0.2800 - val_loss: 2.0245 - val_acc: 0.3050\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0419 - acc: 0.2970 - val_loss: 2.0149 - val_acc: 0.3000\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0512 - acc: 0.2800 - val_loss: 2.0141 - val_acc: 0.3200\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0394 - acc: 0.2830 - val_loss: 2.0359 - val_acc: 0.3300\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0251 - acc: 0.2850 - val_loss: 2.0178 - val_acc: 0.3200\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0406 - acc: 0.2660 - val_loss: 2.0539 - val_acc: 0.2800\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0505 - acc: 0.2760 - val_loss: 2.0116 - val_acc: 0.2800\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0313 - acc: 0.2820 - val_loss: 2.0123 - val_acc: 0.3300\n",
      "Epoch 30/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0378 - acc: 0.2850 - val_loss: 1.9921 - val_acc: 0.3050\n",
      "Epoch 31/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0384 - acc: 0.2810 - val_loss: 2.0163 - val_acc: 0.3450\n",
      "Epoch 32/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0219 - acc: 0.2810 - val_loss: 2.0052 - val_acc: 0.3500\n",
      "Epoch 33/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0583 - acc: 0.2650 - val_loss: 2.0069 - val_acc: 0.3200\n",
      "Epoch 34/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0549 - acc: 0.2700 - val_loss: 2.0430 - val_acc: 0.2850\n",
      "Epoch 35/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0322 - acc: 0.2680 - val_loss: 1.9942 - val_acc: 0.3400\n",
      "Epoch 36/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0264 - acc: 0.2900 - val_loss: 2.0253 - val_acc: 0.2900\n",
      "Epoch 37/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0305 - acc: 0.2910 - val_loss: 2.0102 - val_acc: 0.3100\n",
      "Epoch 38/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0133 - acc: 0.2980 - val_loss: 1.9961 - val_acc: 0.3150\n",
      "Epoch 39/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0311 - acc: 0.2460 - val_loss: 2.0111 - val_acc: 0.3050\n",
      "Epoch 40/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0005 - acc: 0.2650 - val_loss: 1.9873 - val_acc: 0.3200\n",
      "Epoch 41/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0458 - acc: 0.2650 - val_loss: 1.9972 - val_acc: 0.3000\n",
      "Epoch 42/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0118 - acc: 0.2730 - val_loss: 1.9882 - val_acc: 0.3050\n",
      "Epoch 43/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0399 - acc: 0.2740 - val_loss: 1.9831 - val_acc: 0.3350\n",
      "Epoch 44/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0265 - acc: 0.2760 - val_loss: 2.0499 - val_acc: 0.2900\n",
      "Epoch 45/50\n",
      "1000/1000 [==============================] - 0s - loss: 1.9981 - acc: 0.2870 - val_loss: 1.9704 - val_acc: 0.3400\n",
      "Epoch 46/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0171 - acc: 0.2700 - val_loss: 2.0035 - val_acc: 0.3000\n",
      "Epoch 47/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0308 - acc: 0.3020 - val_loss: 1.9693 - val_acc: 0.3350\n",
      "Epoch 48/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0041 - acc: 0.2760 - val_loss: 2.0146 - val_acc: 0.3150\n",
      "Epoch 49/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0236 - acc: 0.2470 - val_loss: 1.9818 - val_acc: 0.3200\n",
      "Epoch 50/50\n",
      "1000/1000 [==============================] - 0s - loss: 2.0063 - acc: 0.2800 - val_loss: 1.9969 - val_acc: 0.2950\n",
      "Test loss: 1.99685703278\n",
      "Test accuracy: 0.295\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=50,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
