{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrafi123/anaconda3/lib/python3.5/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.\n",
      "  warnings.warn(\"Your cuDNN version is more recent than \"\n",
      "Using cuDNN version 6021 on context None\n",
      "Mapped name None to device cuda: GeForce 920M (0000:08:00.0)\n"
     ]
    }
   ],
   "source": [
    "import os    \n",
    "#os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu0,floatX=float32\"\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=cuda,floatX=float32\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, UpSampling3D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, Deconvolution2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "from keras.layers.core import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "import keras.regularizers\n",
    "\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<GpuArrayType<None>(float32, (False,))>), HostFromGpu(gpuarray)(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.874952 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "#test for gpu\n",
    "from theano import function, config, shared, tensor\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], tensor.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, tensor.Elemwise) and\n",
    "              ('Gpu' not in type(x.op).__name__)\n",
    "              for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l_images_train, u_images_train, l_labels_train, u_labels_train = train_test_split(\n",
    "    x_train,y_train,test_size=59000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(59000, 28, 28)\n",
      "(1000,)\n",
      "(59000,)\n"
     ]
    }
   ],
   "source": [
    "print (l_images_train.shape)\n",
    "print (u_images_train.shape)\n",
    "print (l_labels_train.shape)\n",
    "print (u_labels_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 105, 1: 114, 2: 107, 3: 92, 4: 90, 5: 83, 6: 103, 7: 100, 8: 102, 9: 104}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = numpy.unique(l_labels_train, return_counts=True)\n",
    "print (dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(l_images_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  22 151\n",
      "  223 238  56   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  74 214 236\n",
      "  134  96  19   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 104 248 212  49\n",
      "    0  43   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  57 245 210  22   0\n",
      "   66 221  15   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  61 232 207  54   0   0\n",
      "  216 209   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  25 207 236  56   0   0  51\n",
      "  248 209   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 153 251 111   0   0   0 101\n",
      "  253 130   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  33 240 169   0   0   0  36 222\n",
      "  231  20   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 123 253  85   0   0  31 224 251\n",
      "  103   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 183 204  19   0  31 169 253 191\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  72 254 139   0  91 227 255 254  48\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  77 253 219 245 251 253 254 185   1\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   3 161 253 206 122 226 235  48   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  13  26   7 115 253 175   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  31 243 238  54   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 118 252 113   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  21 219 205   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0 120 253 121   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2 203 237  19   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  96 253 148   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print (l_images_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l_images_train = l_images_train.astype('float32')\n",
    "l_images_train /= 255\n",
    "print(l_images_train.shape)\n",
    "\n",
    "print (l_images_train.shape)\n",
    "l_images_train = l_images_train.reshape(-1,1, 28, 28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print (l_images_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print (l_labels_train.shape)\n",
    "l_labels_train = l_labels_train.reshape(1000,1)\n",
    "print (l_labels_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9]\n",
      " [4]\n",
      " [2]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [0]\n",
      " [9]\n",
      " [7]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [8]\n",
      " [6]\n",
      " [8]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [7]\n",
      " [0]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [6]\n",
      " [7]\n",
      " [4]\n",
      " [6]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [7]\n",
      " [9]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [9]\n",
      " [8]\n",
      " [7]\n",
      " [9]\n",
      " [4]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [0]\n",
      " [5]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [8]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [3]\n",
      " [8]\n",
      " [4]\n",
      " [7]\n",
      " [3]\n",
      " [4]\n",
      " [8]\n",
      " [4]\n",
      " [8]\n",
      " [0]\n",
      " [7]\n",
      " [7]\n",
      " [8]\n",
      " [3]\n",
      " [7]\n",
      " [9]\n",
      " [2]\n",
      " [1]\n",
      " [6]\n",
      " [7]\n",
      " [5]\n",
      " [0]\n",
      " [2]\n",
      " [5]\n",
      " [7]\n",
      " [3]\n",
      " [4]\n",
      " [4]\n",
      " [5]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [6]\n",
      " [9]\n",
      " [5]\n",
      " [5]\n",
      " [6]\n",
      " [2]\n",
      " [6]\n",
      " [0]\n",
      " [8]\n",
      " [9]\n",
      " [7]\n",
      " [9]\n",
      " [1]\n",
      " [7]\n",
      " [4]\n",
      " [2]\n",
      " [0]\n",
      " [7]\n",
      " [5]\n",
      " [3]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [8]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [3]\n",
      " [5]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [5]\n",
      " [2]\n",
      " [5]\n",
      " [4]\n",
      " [4]\n",
      " [4]\n",
      " [2]\n",
      " [2]\n",
      " [9]\n",
      " [4]\n",
      " [9]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [5]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [7]\n",
      " [8]\n",
      " [5]\n",
      " [2]\n",
      " [2]\n",
      " [7]\n",
      " [4]\n",
      " [8]\n",
      " [1]\n",
      " [9]\n",
      " [6]\n",
      " [5]\n",
      " [7]\n",
      " [2]\n",
      " [6]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [0]\n",
      " [6]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [6]\n",
      " [1]\n",
      " [5]\n",
      " [8]\n",
      " [1]\n",
      " [8]\n",
      " [3]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [5]\n",
      " [8]\n",
      " [9]\n",
      " [7]\n",
      " [9]\n",
      " [8]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [8]\n",
      " [1]\n",
      " [8]\n",
      " [5]\n",
      " [7]\n",
      " [2]\n",
      " [0]\n",
      " [4]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [6]\n",
      " [3]\n",
      " [6]\n",
      " [4]\n",
      " [1]\n",
      " [5]\n",
      " [2]\n",
      " [9]\n",
      " [6]\n",
      " [7]\n",
      " [1]\n",
      " [7]\n",
      " [3]\n",
      " [1]\n",
      " [9]\n",
      " [7]\n",
      " [1]\n",
      " [0]\n",
      " [6]\n",
      " [9]\n",
      " [7]\n",
      " [3]\n",
      " [1]\n",
      " [8]\n",
      " [1]\n",
      " [6]\n",
      " [5]\n",
      " [1]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [1]\n",
      " [2]\n",
      " [4]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [5]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [0]\n",
      " [3]\n",
      " [4]\n",
      " [8]\n",
      " [7]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [7]\n",
      " [0]\n",
      " [8]\n",
      " [9]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [4]\n",
      " [5]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [4]\n",
      " [5]\n",
      " [6]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [7]\n",
      " [9]\n",
      " [4]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [9]\n",
      " [7]\n",
      " [6]\n",
      " [1]\n",
      " [6]\n",
      " [9]\n",
      " [2]\n",
      " [6]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [8]\n",
      " [0]\n",
      " [6]\n",
      " [5]\n",
      " [4]\n",
      " [9]\n",
      " [3]\n",
      " [7]\n",
      " [6]\n",
      " [1]\n",
      " [0]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [7]\n",
      " [1]\n",
      " [3]\n",
      " [9]\n",
      " [4]\n",
      " [3]\n",
      " [4]\n",
      " [4]\n",
      " [9]\n",
      " [4]\n",
      " [2]\n",
      " [9]\n",
      " [9]\n",
      " [9]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [1]\n",
      " [6]\n",
      " [0]\n",
      " [1]\n",
      " [6]\n",
      " [0]\n",
      " [8]\n",
      " [6]\n",
      " [1]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [1]\n",
      " [9]\n",
      " [5]\n",
      " [2]\n",
      " [6]\n",
      " [7]\n",
      " [3]\n",
      " [2]\n",
      " [6]\n",
      " [3]\n",
      " [6]\n",
      " [1]\n",
      " [6]\n",
      " [1]\n",
      " [0]\n",
      " [9]\n",
      " [8]\n",
      " [1]\n",
      " [5]\n",
      " [9]\n",
      " [2]\n",
      " [0]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [8]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [9]\n",
      " [8]\n",
      " [6]\n",
      " [9]\n",
      " [1]\n",
      " [9]\n",
      " [0]\n",
      " [0]\n",
      " [9]\n",
      " [3]\n",
      " [5]\n",
      " [8]\n",
      " [9]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [8]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [0]\n",
      " [6]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [9]\n",
      " [9]\n",
      " [8]\n",
      " [6]\n",
      " [3]\n",
      " [2]\n",
      " [2]\n",
      " [7]\n",
      " [0]\n",
      " [8]\n",
      " [6]\n",
      " [1]\n",
      " [8]\n",
      " [9]\n",
      " [1]\n",
      " [3]\n",
      " [3]\n",
      " [9]\n",
      " [3]\n",
      " [9]\n",
      " [2]\n",
      " [5]\n",
      " [7]\n",
      " [8]\n",
      " [7]\n",
      " [6]\n",
      " [5]\n",
      " [8]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [5]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [5]\n",
      " [0]\n",
      " [4]\n",
      " [0]\n",
      " [8]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [2]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [8]\n",
      " [2]\n",
      " [0]\n",
      " [7]\n",
      " [5]\n",
      " [6]\n",
      " [5]\n",
      " [5]\n",
      " [5]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [6]\n",
      " [6]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [1]\n",
      " [6]\n",
      " [5]\n",
      " [9]\n",
      " [0]\n",
      " [4]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [8]\n",
      " [4]\n",
      " [0]\n",
      " [6]\n",
      " [6]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [3]\n",
      " [4]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [7]\n",
      " [4]\n",
      " [0]\n",
      " [0]\n",
      " [9]\n",
      " [5]\n",
      " [6]\n",
      " [0]\n",
      " [9]\n",
      " [3]\n",
      " [3]\n",
      " [6]\n",
      " [5]\n",
      " [6]\n",
      " [5]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [5]\n",
      " [8]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [2]\n",
      " [8]\n",
      " [8]\n",
      " [3]\n",
      " [1]\n",
      " [0]\n",
      " [8]\n",
      " [4]\n",
      " [5]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [1]\n",
      " [4]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [0]\n",
      " [2]\n",
      " [7]\n",
      " [9]\n",
      " [2]\n",
      " [9]\n",
      " [6]\n",
      " [7]\n",
      " [2]\n",
      " [8]\n",
      " [5]\n",
      " [7]\n",
      " [8]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [3]\n",
      " [0]\n",
      " [4]\n",
      " [1]\n",
      " [2]\n",
      " [9]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [9]\n",
      " [3]\n",
      " [1]\n",
      " [9]\n",
      " [7]\n",
      " [3]\n",
      " [8]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [5]\n",
      " [9]\n",
      " [8]\n",
      " [1]\n",
      " [9]\n",
      " [8]\n",
      " [9]\n",
      " [9]\n",
      " [8]\n",
      " [2]\n",
      " [6]\n",
      " [5]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [3]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [5]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [6]\n",
      " [9]\n",
      " [5]\n",
      " [5]\n",
      " [3]\n",
      " [8]\n",
      " [3]\n",
      " [2]\n",
      " [5]\n",
      " [9]\n",
      " [7]\n",
      " [7]\n",
      " [6]\n",
      " [9]\n",
      " [9]\n",
      " [7]\n",
      " [8]\n",
      " [1]\n",
      " [0]\n",
      " [7]\n",
      " [4]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [2]\n",
      " [5]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [0]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [5]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [2]\n",
      " [6]\n",
      " [3]\n",
      " [3]\n",
      " [9]\n",
      " [6]\n",
      " [2]\n",
      " [3]\n",
      " [2]\n",
      " [8]\n",
      " [6]\n",
      " [3]\n",
      " [0]\n",
      " [8]\n",
      " [3]\n",
      " [1]\n",
      " [9]\n",
      " [6]\n",
      " [8]\n",
      " [3]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [6]\n",
      " [8]\n",
      " [1]\n",
      " [1]\n",
      " [5]\n",
      " [6]\n",
      " [7]\n",
      " [9]\n",
      " [8]\n",
      " [8]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [7]\n",
      " [1]\n",
      " [6]\n",
      " [8]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [9]\n",
      " [0]\n",
      " [2]\n",
      " [5]\n",
      " [5]\n",
      " [9]\n",
      " [5]\n",
      " [9]\n",
      " [8]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [0]\n",
      " [9]\n",
      " [8]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [7]\n",
      " [2]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [5]\n",
      " [4]\n",
      " [7]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [5]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [6]\n",
      " [5]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [4]\n",
      " [0]\n",
      " [1]\n",
      " [8]\n",
      " [2]\n",
      " [9]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [6]\n",
      " [8]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [2]\n",
      " [8]\n",
      " [5]\n",
      " [5]\n",
      " [8]\n",
      " [0]\n",
      " [4]\n",
      " [8]\n",
      " [7]\n",
      " [8]\n",
      " [8]\n",
      " [2]\n",
      " [2]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [2]\n",
      " [3]\n",
      " [3]\n",
      " [9]\n",
      " [2]\n",
      " [5]\n",
      " [9]\n",
      " [1]\n",
      " [6]\n",
      " [0]\n",
      " [2]\n",
      " [6]\n",
      " [8]\n",
      " [0]\n",
      " [3]\n",
      " [0]\n",
      " [6]\n",
      " [5]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [3]\n",
      " [8]\n",
      " [5]\n",
      " [8]\n",
      " [9]\n",
      " [8]\n",
      " [3]\n",
      " [6]\n",
      " [4]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [7]\n",
      " [0]\n",
      " [8]\n",
      " [9]\n",
      " [9]\n",
      " [2]\n",
      " [2]\n",
      " [8]\n",
      " [1]\n",
      " [4]\n",
      " [7]\n",
      " [6]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [5]\n",
      " [6]\n",
      " [9]\n",
      " [7]\n",
      " [2]\n",
      " [6]\n",
      " [1]\n",
      " [0]\n",
      " [3]\n",
      " [6]\n",
      " [4]\n",
      " [7]\n",
      " [6]\n",
      " [6]\n",
      " [1]\n",
      " [5]\n",
      " [7]\n",
      " [2]\n",
      " [2]\n",
      " [7]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [4]\n",
      " [6]\n",
      " [0]\n",
      " [3]\n",
      " [2]\n",
      " [7]\n",
      " [6]\n",
      " [1]\n",
      " [7]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [4]\n",
      " [6]\n",
      " [2]\n",
      " [8]\n",
      " [4]\n",
      " [7]\n",
      " [7]\n",
      " [3]\n",
      " [7]\n",
      " [7]\n",
      " [0]\n",
      " [3]\n",
      " [1]\n",
      " [1]\n",
      " [9]\n",
      " [6]\n",
      " [0]\n",
      " [5]\n",
      " [4]\n",
      " [8]\n",
      " [8]\n",
      " [7]\n",
      " [7]\n",
      " [7]\n",
      " [2]\n",
      " [3]\n",
      " [8]\n",
      " [3]\n",
      " [2]\n",
      " [1]\n",
      " [8]\n",
      " [3]\n",
      " [6]\n",
      " [6]\n",
      " [0]\n",
      " [3]\n",
      " [3]\n",
      " [4]\n",
      " [3]\n",
      " [9]\n",
      " [3]\n",
      " [0]\n",
      " [9]\n",
      " [1]\n",
      " [7]\n",
      " [1]\n",
      " [9]\n",
      " [8]\n",
      " [5]\n",
      " [4]\n",
      " [8]\n",
      " [3]\n",
      " [3]\n",
      " [7]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [7]\n",
      " [3]\n",
      " [4]\n",
      " [9]\n",
      " [2]\n",
      " [5]\n",
      " [1]\n",
      " [6]\n",
      " [1]\n",
      " [4]\n",
      " [9]\n",
      " [3]\n",
      " [0]\n",
      " [2]\n",
      " [7]\n",
      " [2]\n",
      " [3]\n",
      " [9]\n",
      " [8]\n",
      " [4]\n",
      " [7]\n",
      " [9]\n",
      " [7]\n",
      " [6]\n",
      " [2]\n",
      " [1]\n",
      " [4]\n",
      " [1]\n",
      " [5]\n",
      " [8]\n",
      " [1]\n",
      " [2]\n",
      " [9]\n",
      " [3]\n",
      " [6]\n",
      " [3]\n",
      " [0]\n",
      " [1]\n",
      " [4]\n",
      " [6]\n",
      " [7]\n",
      " [6]\n",
      " [7]\n",
      " [0]\n",
      " [3]\n",
      " [8]\n",
      " [4]\n",
      " [1]\n",
      " [8]\n",
      " [1]\n",
      " [6]\n",
      " [8]\n",
      " [6]\n",
      " [4]\n",
      " [6]\n",
      " [1]\n",
      " [3]\n",
      " [6]\n",
      " [3]\n",
      " [8]\n",
      " [0]\n",
      " [0]\n",
      " [4]]\n"
     ]
    }
   ],
   "source": [
    "print (l_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_labels_train = np_utils.to_categorical(l_labels_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print (l_labels_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "\n",
    "input_shape = (784,)\n",
    "input_reshape = (1,28, 28)\n",
    "\n",
    "conv_num_filters = 5\n",
    "conv_filter_size = 5\n",
    "\n",
    "pool_size = (2, 2)\n",
    "\n",
    "hidden_num_units = 50\n",
    "output_num_units = 10\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_layers = [\n",
    "    Convolution2D(25, 5, 5, border_mode='same', input_shape=(input_reshape), activation='relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Convolution2D(25, 5, 5, border_mode='same',  activation='relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Convolution2D(25, 4, 4, border_mode='same', activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(output_dim=hidden_num_units, activation='relu')\n",
    "]\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(10, activation='softmax', W_regularizer=keras.regularizers.l2(0.01), name='fc_layer2')\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential(feature_layers + classification_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_7 (Convolution2D)  (None, 25, 28, 28)    650         convolution2d_input_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 25, 14, 14)    0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 25, 14, 14)    15650       maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 25, 7, 7)      0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 25, 7, 7)      10025       maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 1225)          0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 50)            61300       flatten_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fc_layer2 (Dense)                (None, 10)            510         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 88,135\n",
      "Trainable params: 88,135\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Compilation successful\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Model Compilation successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3923 - acc: 0.2720     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.8704 - acc: 0.5410     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.0900 - acc: 0.7400     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.7362 - acc: 0.8180     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.5942 - acc: 0.8560     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.5101 - acc: 0.8860     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.4312 - acc: 0.9170     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.3736 - acc: 0.9300     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.3309 - acc: 0.9490     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 0.2986 - acc: 0.9600     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26b0c76ef0>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(l_images_train, l_labels_train, batch_size=batch_size, nb_epoch=10,\n",
    "          verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "x_test = x_test.astype('float32')\n",
    "x_test /= 255\n",
    "print(x_test.shape)\n",
    "\n",
    "print (x_test.shape)\n",
    "x_test = x_test.reshape(-1,1, 28, 28)\n",
    "print (x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.395036410713\n",
      "Test accuracy: 0.9233\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.45490196,  0.49019608,  0.67058825,  1.        ,  1.        ,\n",
       "          0.58823532,  0.36470589,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.66274512,\n",
       "          0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "          0.99215686,  0.85490197,  0.11764706,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.66274512,  0.99215686,\n",
       "          0.99215686,  0.99215686,  0.83529413,  0.55686277,  0.6901961 ,\n",
       "          0.99215686,  0.99215686,  0.47843137,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.20392157,  0.98039216,  0.99215686,\n",
       "          0.82352942,  0.1254902 ,  0.04705882,  0.        ,  0.02352941,\n",
       "          0.80784315,  0.99215686,  0.54901963,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.3019608 ,  0.98431373,  0.82352942,\n",
       "          0.09803922,  0.        ,  0.        ,  0.        ,  0.47843137,\n",
       "          0.97254902,  0.99215686,  0.25490198,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.12156863,  0.07058824,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.81960785,\n",
       "          0.99215686,  0.99215686,  0.25490198,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.45882353,  0.96862745,\n",
       "          0.99215686,  0.7764706 ,  0.03921569,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.29803923,  0.96862745,  0.99215686,\n",
       "          0.90588236,  0.24705882,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.50196081,  0.99215686,  0.99215686,\n",
       "          0.56470591,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.6901961 ,  0.96470588,  0.99215686,  0.62352943,\n",
       "          0.04705882,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.09803922,  0.91764706,  0.99215686,  0.9137255 ,  0.13725491,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.7764706 ,  0.99215686,  0.99215686,  0.5529412 ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.30588236,\n",
       "          0.97254902,  0.99215686,  0.74117649,  0.04705882,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.07450981,  0.78431374,\n",
       "          0.99215686,  0.99215686,  0.5529412 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.52549022,  0.99215686,\n",
       "          0.99215686,  0.67843139,  0.04705882,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.97254902,  0.99215686,\n",
       "          0.99215686,  0.09803922,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.97254902,  0.99215686,\n",
       "          0.99215686,  0.16862746,  0.07843138,  0.07843138,  0.07843138,\n",
       "          0.07843138,  0.01960784,  0.        ,  0.01960784,  0.07843138,\n",
       "          0.07843138,  0.14509805,  0.58823532,  0.58823532,  0.58823532,\n",
       "          0.57647061,  0.03921569,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.97254902,  0.99215686,\n",
       "          0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "          0.99215686,  0.65882355,  0.56078434,  0.65098041,  0.99215686,\n",
       "          0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "          0.99215686,  0.48235294,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.68235296,  0.99215686,\n",
       "          0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "          0.99215686,  0.99215686,  0.99215686,  0.99215686,  0.99215686,\n",
       "          0.97647059,  0.96862745,  0.96862745,  0.66274512,  0.45882353,\n",
       "          0.45882353,  0.22352941,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.4627451 ,\n",
       "          0.48235294,  0.48235294,  0.48235294,  0.65098041,  0.99215686,\n",
       "          0.99215686,  0.99215686,  0.60784316,  0.48235294,  0.48235294,\n",
       "          0.16078432,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.00788179e-05,   4.78546781e-07,   7.58414387e-07,\n",
       "         1.33511621e-05,   9.07759379e-09,   4.69311459e-07,\n",
       "         8.42060810e-10,   9.99808729e-01,   3.32009472e-06,\n",
       "         1.42846038e-04], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.54258627e-05,   1.23669146e-08,   6.35241298e-03,\n",
       "         9.92520213e-01,   1.63691860e-09,   3.26768495e-05,\n",
       "         1.49934393e-04,   3.34126782e-10,   9.08836257e-04,\n",
       "         5.24322274e-07], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_pred[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_new = y_pred[:, :10]  # First 10 columns of full_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.07653146e-06,   6.06766029e-04,   9.63414073e-01,\n",
       "         2.74561439e-03,   7.47198392e-09,   1.99802544e-05,\n",
       "         3.31398025e-02,   4.94708718e-10,   6.76963245e-05,\n",
       "         1.07124558e-08], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_new = np.ndarray.argmax(y_pred_new,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_new = np.ndarray.argmax(y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tested  10000 digits\n",
      "correct:  9872 wrong:  128 error rate:  1.28 %\n",
      "got correctly  98.72 %\n"
     ]
    }
   ],
   "source": [
    "num=10000\n",
    "r=0\n",
    "w=0\n",
    "for i in range(num):\n",
    "        #print ('y_pred ',y_pred[i])\n",
    "        #print ('labels ',labels[i])\n",
    "        #without the use of all() returns error truth value of an array with more than one element is ambiguous\n",
    "        if y_pred_new[i].all() == y_test_new[i].all():\n",
    "        #if np.array_equal(y_pred[i],y_test[i]):\n",
    "            r+=1\n",
    "        else:\n",
    "            w+=1\n",
    "print (\"tested \",  num, \"digits\")\n",
    "print (\"correct: \", r, \"wrong: \", w, \"error rate: \", float(w)*100/(r+w), \"%\")\n",
    "print (\"got correctly \", float(r)*100/(r+w), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
